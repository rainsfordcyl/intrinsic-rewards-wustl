Mon Apr  7 12:55:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce GTX 1080 Ti     Off |   00000000:1A:00.0 Off |                  N/A |
| 21%   28C    P8             10W /  250W |       3MiB /  11264MiB |      0%   E. Process |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 593      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 2048     |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.02e+03      |
|    ep_rew_mean          | 0             |
| time/                   |               |
|    fps                  | 511           |
|    iterations           | 2             |
|    time_elapsed         | 8             |
|    total_timesteps      | 4096          |
| train/                  |               |
|    approx_kl            | 0.00026599475 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.95         |
|    explained_variance   | -8.85         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00814      |
|    n_updates            | 10            |
|    policy_gradient_loss | -8.68e-05     |
|    value_loss           | 1.92e-06      |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 496         |
|    iterations           | 3           |
|    time_elapsed         | 12          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.011145228 |
|    clip_fraction        | 0.0482      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -4.28       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0141     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00489    |
|    value_loss           | 1.65e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 489         |
|    iterations           | 4           |
|    time_elapsed         | 16          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.011044081 |
|    clip_fraction        | 0.0841      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | -5.56       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0521     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00698    |
|    value_loss           | 1.55e-05    |
-----------------------------------------

Evaluation at step 10000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 321         |
|    iterations           | 5           |
|    time_elapsed         | 31          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.008677218 |
|    clip_fraction        | 0.0588      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -7.19       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0148     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00757    |
|    value_loss           | 1.28e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 6           |
|    time_elapsed         | 36          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.011143537 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | -3.45       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00155    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00857    |
|    value_loss           | 1.32e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 352         |
|    iterations           | 7           |
|    time_elapsed         | 40          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.014148435 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | -2.79       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00892    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00871    |
|    value_loss           | 1.56e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 364         |
|    iterations           | 8           |
|    time_elapsed         | 44          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.012558859 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | -4.4        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0465     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 1.51e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 373         |
|    iterations           | 9           |
|    time_elapsed         | 49          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.018354211 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | -3.31       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0293     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00908    |
|    value_loss           | 1.35e-05    |
-----------------------------------------

Evaluation at step 20000: mean reward = 0.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.01e+03  |
|    ep_rew_mean          | 0.017     |
| time/                   |           |
|    fps                  | 318       |
|    iterations           | 10        |
|    time_elapsed         | 64        |
|    total_timesteps      | 20480     |
| train/                  |           |
|    approx_kl            | 0.0095119 |
|    clip_fraction        | 0.0793    |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.86     |
|    explained_variance   | -6.27     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0166   |
|    n_updates            | 90        |
|    policy_gradient_loss | -0.00895  |
|    value_loss           | 1.45e-05  |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 0.0155     |
| time/                   |            |
|    fps                  | 326        |
|    iterations           | 11         |
|    time_elapsed         | 68         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.01050389 |
|    clip_fraction        | 0.0977     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.84      |
|    explained_variance   | -0.0398    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0272    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.00652   |
|    value_loss           | 0.000545   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0142      |
| time/                   |             |
|    fps                  | 335         |
|    iterations           | 12          |
|    time_elapsed         | 73          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.014679061 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | -4.1        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00326     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 4.19e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0131      |
| time/                   |             |
|    fps                  | 342         |
|    iterations           | 13          |
|    time_elapsed         | 77          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.013642362 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | -2.02       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00509     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 1.57e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0122      |
| time/                   |             |
|    fps                  | 350         |
|    iterations           | 14          |
|    time_elapsed         | 81          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.019734742 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | -1.94       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00395    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 1.71e-05    |
-----------------------------------------

Evaluation at step 30000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0114      |
| time/                   |             |
|    fps                  | 317         |
|    iterations           | 15          |
|    time_elapsed         | 96          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.011125471 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | -1.32       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0136     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 1.17e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0.0107      |
| time/                   |             |
|    fps                  | 324         |
|    iterations           | 16          |
|    time_elapsed         | 101         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.011026636 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | -4.07       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00604    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 8.92e-06    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 0.01       |
| time/                   |            |
|    fps                  | 330        |
|    iterations           | 17         |
|    time_elapsed         | 105        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.01039469 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.76      |
|    explained_variance   | -4.04      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0172    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0067    |
|    value_loss           | 8.45e-06   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0.00947     |
| time/                   |             |
|    fps                  | 336         |
|    iterations           | 18          |
|    time_elapsed         | 109         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.008714411 |
|    clip_fraction        | 0.0704      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | -6.71       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0388     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 1.89e-05    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 0.00897    |
| time/                   |            |
|    fps                  | 341        |
|    iterations           | 19         |
|    time_elapsed         | 113        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.00959715 |
|    clip_fraction        | 0.0849     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.79      |
|    explained_variance   | -1.52      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0223    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.00939   |
|    value_loss           | 8.7e-06    |
----------------------------------------

Evaluation at step 40000: mean reward = 0.00
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 0.00852    |
| time/                   |            |
|    fps                  | 318        |
|    iterations           | 20         |
|    time_elapsed         | 128        |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.01344692 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.77      |
|    explained_variance   | -0.644     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0115    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0133    |
|    value_loss           | 8.92e-06   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0.00811     |
| time/                   |             |
|    fps                  | 323         |
|    iterations           | 21          |
|    time_elapsed         | 132         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.013973709 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | -2.43       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0368     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00842    |
|    value_loss           | 1.56e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0204      |
| time/                   |             |
|    fps                  | 328         |
|    iterations           | 22          |
|    time_elapsed         | 137         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.011411567 |
|    clip_fraction        | 0.0798      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | -1.58       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0237      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 1.12e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0229      |
| time/                   |             |
|    fps                  | 332         |
|    iterations           | 23          |
|    time_elapsed         | 141         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.007824266 |
|    clip_fraction        | 0.0888      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.0384     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0143     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00414    |
|    value_loss           | 0.00134     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.022       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 24          |
|    time_elapsed         | 145         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.008102602 |
|    clip_fraction        | 0.0832      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.446      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00854    |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00639    |
|    value_loss           | 0.000123    |
-----------------------------------------

Evaluation at step 50000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 997         |
|    ep_rew_mean          | 0.0319      |
| time/                   |             |
|    fps                  | 318         |
|    iterations           | 25          |
|    time_elapsed         | 160         |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.014540743 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -0.33       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0083      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 2.3e-05     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 998         |
|    ep_rew_mean          | 0.0307      |
| time/                   |             |
|    fps                  | 322         |
|    iterations           | 26          |
|    time_elapsed         | 164         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.010115551 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -0.00566    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00486     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 0.00126     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 987        |
|    ep_rew_mean          | 0.0431     |
| time/                   |            |
|    fps                  | 326        |
|    iterations           | 27         |
|    time_elapsed         | 169        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.01000941 |
|    clip_fraction        | 0.0867     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.74      |
|    explained_variance   | -1.07      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0316    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.00825   |
|    value_loss           | 0.0001     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 966         |
|    ep_rew_mean          | 0.0642      |
| time/                   |             |
|    fps                  | 330         |
|    iterations           | 28          |
|    time_elapsed         | 173         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.012227876 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.0326      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0465     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00632    |
|    value_loss           | 0.00137     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 956         |
|    ep_rew_mean          | 0.0756      |
| time/                   |             |
|    fps                  | 334         |
|    iterations           | 29          |
|    time_elapsed         | 177         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.010919774 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.0578      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0102     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.00366     |
-----------------------------------------

Evaluation at step 60000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 943         |
|    ep_rew_mean          | 0.0896      |
| time/                   |             |
|    fps                  | 317         |
|    iterations           | 30          |
|    time_elapsed         | 193         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.009481461 |
|    clip_fraction        | 0.0959      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.214       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0472     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00781    |
|    value_loss           | 0.00119     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 917         |
|    ep_rew_mean          | 0.118       |
| time/                   |             |
|    fps                  | 320         |
|    iterations           | 31          |
|    time_elapsed         | 197         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.011428667 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.0479      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0178     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00984    |
|    value_loss           | 0.00232     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 839        |
|    ep_rew_mean          | 0.194      |
| time/                   |            |
|    fps                  | 324        |
|    iterations           | 32         |
|    time_elapsed         | 202        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.01276515 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.67      |
|    explained_variance   | 0.122      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0121    |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 0.00549    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 750         |
|    ep_rew_mean          | 0.282       |
| time/                   |             |
|    fps                  | 327         |
|    iterations           | 33          |
|    time_elapsed         | 206         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.012530824 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.162       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.037      |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.0152      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 603         |
|    ep_rew_mean          | 0.426       |
| time/                   |             |
|    fps                  | 330         |
|    iterations           | 34          |
|    time_elapsed         | 210         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.013306112 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00183    |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 0.0199      |
-----------------------------------------

Evaluation at step 70000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 381         |
|    ep_rew_mean          | 0.644       |
| time/                   |             |
|    fps                  | 317         |
|    iterations           | 35          |
|    time_elapsed         | 225         |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.012046103 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0235     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.0235      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 151         |
|    ep_rew_mean          | 0.866       |
| time/                   |             |
|    fps                  | 320         |
|    iterations           | 36          |
|    time_elapsed         | 230         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.013787758 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.417       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00563     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.0208      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 78.6         |
|    ep_rew_mean          | 0.931        |
| time/                   |              |
|    fps                  | 323          |
|    iterations           | 37           |
|    time_elapsed         | 234          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0136557855 |
|    clip_fraction        | 0.149        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.42        |
|    explained_variance   | 0.537        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0415      |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.0123      |
|    value_loss           | 0.014        |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 63.7       |
|    ep_rew_mean          | 0.944      |
| time/                   |            |
|    fps                  | 326        |
|    iterations           | 38         |
|    time_elapsed         | 238        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.01593347 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | 0.532      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00204    |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.0173     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 52.6        |
|    ep_rew_mean          | 0.954       |
| time/                   |             |
|    fps                  | 328         |
|    iterations           | 39          |
|    time_elapsed         | 242         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.012346802 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.594       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0111     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00844    |
|    value_loss           | 0.0139      |
-----------------------------------------

Evaluation at step 80000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 47.5        |
|    ep_rew_mean          | 0.958       |
| time/                   |             |
|    fps                  | 331         |
|    iterations           | 40          |
|    time_elapsed         | 247         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.014223109 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00383    |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.0115      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 40.6        |
|    ep_rew_mean          | 0.964       |
| time/                   |             |
|    fps                  | 333         |
|    iterations           | 41          |
|    time_elapsed         | 251         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.010246424 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.999      |
|    explained_variance   | 0.542       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0217     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.01        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.1        |
|    ep_rew_mean          | 0.967       |
| time/                   |             |
|    fps                  | 336         |
|    iterations           | 42          |
|    time_elapsed         | 255         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.008306624 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.837      |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0204     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.00699     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 35.2         |
|    ep_rew_mean          | 0.969        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 43           |
|    time_elapsed         | 260          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0033406555 |
|    clip_fraction        | 0.0428       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.757       |
|    explained_variance   | 0.532        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00578     |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00171     |
|    value_loss           | 0.0054       |
------------------------------------------

Evaluation at step 90000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 34.1        |
|    ep_rew_mean          | 0.97        |
| time/                   |             |
|    fps                  | 340         |
|    iterations           | 44          |
|    time_elapsed         | 264         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.009591728 |
|    clip_fraction        | 0.0951      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.692      |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00696    |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00484    |
|    value_loss           | 0.00386     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 33           |
|    ep_rew_mean          | 0.971        |
| time/                   |              |
|    fps                  | 342          |
|    iterations           | 45           |
|    time_elapsed         | 268          |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.0059762932 |
|    clip_fraction        | 0.0729       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.63        |
|    explained_variance   | 0.543        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0197      |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00493     |
|    value_loss           | 0.00394      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.7        |
|    ep_rew_mean          | 0.972       |
| time/                   |             |
|    fps                  | 344         |
|    iterations           | 46          |
|    time_elapsed         | 273         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.014675831 |
|    clip_fraction        | 0.0717      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00503     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.006      |
|    value_loss           | 0.00348     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30.8         |
|    ep_rew_mean          | 0.973        |
| time/                   |              |
|    fps                  | 347          |
|    iterations           | 47           |
|    time_elapsed         | 277          |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 0.0037138762 |
|    clip_fraction        | 0.0585       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.425       |
|    explained_variance   | 0.576        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00263     |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00547     |
|    value_loss           | 0.00308      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 29.4         |
|    ep_rew_mean          | 0.974        |
| time/                   |              |
|    fps                  | 349          |
|    iterations           | 48           |
|    time_elapsed         | 281          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0024025068 |
|    clip_fraction        | 0.038        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.41        |
|    explained_variance   | 0.595        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00554     |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 0.00292      |
------------------------------------------

Evaluation at step 100000: mean reward = 0.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.8         |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 350          |
|    iterations           | 49           |
|    time_elapsed         | 286          |
|    total_timesteps      | 100352       |
| train/                  |              |
|    approx_kl            | 0.0038879786 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.324       |
|    explained_variance   | 0.571        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0113       |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.0015      |
|    value_loss           | 0.00245      |
------------------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 654      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 2           |
|    time_elapsed         | 7           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009867589 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -8.55       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0244     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00378    |
|    value_loss           | 4.9e-06     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 522         |
|    iterations           | 3           |
|    time_elapsed         | 11          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.011878535 |
|    clip_fraction        | 0.0524      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00587    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00441    |
|    value_loss           | 8.17e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 509         |
|    iterations           | 4           |
|    time_elapsed         | 16          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.011036769 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | -3.61       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0369     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00843    |
|    value_loss           | 1.7e-05     |
-----------------------------------------

Evaluation at step 10000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 329         |
|    iterations           | 5           |
|    time_elapsed         | 31          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.013411451 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | -3.85       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0594     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 1.46e-05    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.02e+03     |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 346          |
|    iterations           | 6            |
|    time_elapsed         | 35           |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0114712175 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | -2.25        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000766     |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00576     |
|    value_loss           | 1.03e-05     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0211      |
| time/                   |             |
|    fps                  | 360         |
|    iterations           | 7           |
|    time_elapsed         | 39          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.009021148 |
|    clip_fraction        | 0.0618      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | -1.94       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0286     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00584    |
|    value_loss           | 1.52e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 978         |
|    ep_rew_mean          | 0.0528      |
| time/                   |             |
|    fps                  | 372         |
|    iterations           | 8           |
|    time_elapsed         | 44          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.009507073 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | -0.0129     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0369     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00923    |
|    value_loss           | 0.00036     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 966         |
|    ep_rew_mean          | 0.0717      |
| time/                   |             |
|    fps                  | 381         |
|    iterations           | 9           |
|    time_elapsed         | 48          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.009401994 |
|    clip_fraction        | 0.065       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.0463      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0425     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0061     |
|    value_loss           | 0.00124     |
-----------------------------------------

Evaluation at step 20000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 927         |
|    ep_rew_mean          | 0.114       |
| time/                   |             |
|    fps                  | 321         |
|    iterations           | 10          |
|    time_elapsed         | 63          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.009420506 |
|    clip_fraction        | 0.0815      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.197       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00134    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 0.000705    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 887        |
|    ep_rew_mean          | 0.156      |
| time/                   |            |
|    fps                  | 331        |
|    iterations           | 11         |
|    time_elapsed         | 67         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.00901755 |
|    clip_fraction        | 0.0995     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.81      |
|    explained_variance   | 0.201      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0323    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.00717   |
|    value_loss           | 0.00176    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 813        |
|    ep_rew_mean          | 0.232      |
| time/                   |            |
|    fps                  | 340        |
|    iterations           | 12         |
|    time_elapsed         | 72         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.01483697 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.81      |
|    explained_variance   | 0.34       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00372    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.01      |
|    value_loss           | 0.00244    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 735         |
|    ep_rew_mean          | 0.31        |
| time/                   |             |
|    fps                  | 346         |
|    iterations           | 13          |
|    time_elapsed         | 76          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.009027412 |
|    clip_fraction        | 0.0742      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.304       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00677    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 0.00533     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 682         |
|    ep_rew_mean          | 0.362       |
| time/                   |             |
|    fps                  | 353         |
|    iterations           | 14          |
|    time_elapsed         | 81          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.009904737 |
|    clip_fraction        | 0.0667      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.402       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0088     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00597    |
|    value_loss           | 0.00818     |
-----------------------------------------

Evaluation at step 30000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 622         |
|    ep_rew_mean          | 0.421       |
| time/                   |             |
|    fps                  | 318         |
|    iterations           | 15          |
|    time_elapsed         | 96          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.013909305 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0359     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.00746     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 555         |
|    ep_rew_mean          | 0.485       |
| time/                   |             |
|    fps                  | 325         |
|    iterations           | 16          |
|    time_elapsed         | 100         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.008920279 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0244     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00551    |
|    value_loss           | 0.0106      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 490         |
|    ep_rew_mean          | 0.547       |
| time/                   |             |
|    fps                  | 331         |
|    iterations           | 17          |
|    time_elapsed         | 105         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.010956742 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0192     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00763    |
|    value_loss           | 0.0145      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 423        |
|    ep_rew_mean          | 0.61       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 18         |
|    time_elapsed         | 109        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.01024657 |
|    clip_fraction        | 0.0934     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.67      |
|    explained_variance   | 0.379      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0208    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.00569   |
|    value_loss           | 0.0138     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 307         |
|    ep_rew_mean          | 0.722       |
| time/                   |             |
|    fps                  | 342         |
|    iterations           | 19          |
|    time_elapsed         | 113         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.010700791 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.533       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0135     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.0174      |
-----------------------------------------

Evaluation at step 40000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 178         |
|    ep_rew_mean          | 0.844       |
| time/                   |             |
|    fps                  | 317         |
|    iterations           | 20          |
|    time_elapsed         | 129         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.010425562 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00583    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.0169      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 111          |
|    ep_rew_mean          | 0.902        |
| time/                   |              |
|    fps                  | 322          |
|    iterations           | 21           |
|    time_elapsed         | 133          |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0077375313 |
|    clip_fraction        | 0.0957       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.5         |
|    explained_variance   | 0.535        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000854    |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00718     |
|    value_loss           | 0.0171       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 79.1        |
|    ep_rew_mean          | 0.93        |
| time/                   |             |
|    fps                  | 327         |
|    iterations           | 22          |
|    time_elapsed         | 137         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.011416819 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0142     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.0184      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 59.6        |
|    ep_rew_mean          | 0.948       |
| time/                   |             |
|    fps                  | 331         |
|    iterations           | 23          |
|    time_elapsed         | 141         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.018706273 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0152      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00824    |
|    value_loss           | 0.0165      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 49.1         |
|    ep_rew_mean          | 0.957        |
| time/                   |              |
|    fps                  | 336          |
|    iterations           | 24           |
|    time_elapsed         | 146          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0122098215 |
|    clip_fraction        | 0.166        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.541        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0495      |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.0104      |
|    value_loss           | 0.015        |
------------------------------------------

Evaluation at step 50000: mean reward = 0.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 45           |
|    ep_rew_mean          | 0.96         |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 25           |
|    time_elapsed         | 150          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0068377075 |
|    clip_fraction        | 0.0961       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0.451        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0141      |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00552     |
|    value_loss           | 0.011        |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 41.6       |
|    ep_rew_mean          | 0.963      |
| time/                   |            |
|    fps                  | 343        |
|    iterations           | 26         |
|    time_elapsed         | 155        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.01681524 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.987     |
|    explained_variance   | 0.291      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0135     |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0119    |
|    value_loss           | 0.00627    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 39.7       |
|    ep_rew_mean          | 0.965      |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 27         |
|    time_elapsed         | 159        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.01045835 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.886     |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0142    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.00697   |
|    value_loss           | 0.00406    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 37.6       |
|    ep_rew_mean          | 0.967      |
| time/                   |            |
|    fps                  | 350        |
|    iterations           | 28         |
|    time_elapsed         | 163        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.00912082 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.813     |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00447    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.00752   |
|    value_loss           | 0.0038     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 34.2        |
|    ep_rew_mean          | 0.97        |
| time/                   |             |
|    fps                  | 353         |
|    iterations           | 29          |
|    time_elapsed         | 167         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.013754481 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0127      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 0.00412     |
-----------------------------------------

Evaluation at step 60000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 33.6        |
|    ep_rew_mean          | 0.97        |
| time/                   |             |
|    fps                  | 356         |
|    iterations           | 30          |
|    time_elapsed         | 172         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.007162875 |
|    clip_fraction        | 0.0702      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.543      |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0126     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00274    |
|    value_loss           | 0.00387     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 33.4         |
|    ep_rew_mean          | 0.971        |
| time/                   |              |
|    fps                  | 359          |
|    iterations           | 31           |
|    time_elapsed         | 176          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0066369306 |
|    clip_fraction        | 0.0765       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.571       |
|    explained_variance   | 0.608        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00314     |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 0.00329      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.5        |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 362         |
|    iterations           | 32          |
|    time_elapsed         | 180         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.008490062 |
|    clip_fraction        | 0.065       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.523      |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00466     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00203    |
|    value_loss           | 0.00294     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.1        |
|    ep_rew_mean          | 0.972       |
| time/                   |             |
|    fps                  | 364         |
|    iterations           | 33          |
|    time_elapsed         | 185         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.019684337 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.512      |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0216     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00636    |
|    value_loss           | 0.00287     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.3        |
|    ep_rew_mean          | 0.972       |
| time/                   |             |
|    fps                  | 367         |
|    iterations           | 34          |
|    time_elapsed         | 189         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.009810602 |
|    clip_fraction        | 0.0809      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.396      |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00902     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00469    |
|    value_loss           | 0.0032      |
-----------------------------------------

Evaluation at step 70000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.2        |
|    ep_rew_mean          | 0.972       |
| time/                   |             |
|    fps                  | 369         |
|    iterations           | 35          |
|    time_elapsed         | 194         |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.006068106 |
|    clip_fraction        | 0.0571      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.359      |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0101     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00048    |
|    value_loss           | 0.00337     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.3        |
|    ep_rew_mean          | 0.972       |
| time/                   |             |
|    fps                  | 371         |
|    iterations           | 36          |
|    time_elapsed         | 198         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.009095874 |
|    clip_fraction        | 0.0688      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.372      |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.0003      |
|    loss                 | -8.91e-05   |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0074     |
|    value_loss           | 0.0034      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30          |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 373         |
|    iterations           | 37          |
|    time_elapsed         | 202         |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.016470801 |
|    clip_fraction        | 0.0585      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.295      |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0284     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00825    |
|    value_loss           | 0.00266     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30.5         |
|    ep_rew_mean          | 0.973        |
| time/                   |              |
|    fps                  | 376          |
|    iterations           | 38           |
|    time_elapsed         | 206          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0145363435 |
|    clip_fraction        | 0.0586       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.31        |
|    explained_variance   | 0.608        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00776      |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00368     |
|    value_loss           | 0.00267      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30.4         |
|    ep_rew_mean          | 0.973        |
| time/                   |              |
|    fps                  | 378          |
|    iterations           | 39           |
|    time_elapsed         | 211          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0028969506 |
|    clip_fraction        | 0.057        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.31        |
|    explained_variance   | 0.615        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.012       |
|    n_updates            | 380          |
|    policy_gradient_loss | -3.95e-05    |
|    value_loss           | 0.00296      |
------------------------------------------

Evaluation at step 80000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30.6        |
|    ep_rew_mean          | 0.973       |
| time/                   |             |
|    fps                  | 379         |
|    iterations           | 40          |
|    time_elapsed         | 215         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.015972773 |
|    clip_fraction        | 0.0572      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.283      |
|    explained_variance   | 0.578       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0125     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00302    |
|    value_loss           | 0.0028      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30.6         |
|    ep_rew_mean          | 0.973        |
| time/                   |              |
|    fps                  | 381          |
|    iterations           | 41           |
|    time_elapsed         | 220          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0024791253 |
|    clip_fraction        | 0.052        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.278       |
|    explained_variance   | 0.625        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00159     |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00175     |
|    value_loss           | 0.00275      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30.4        |
|    ep_rew_mean          | 0.973       |
| time/                   |             |
|    fps                  | 383         |
|    iterations           | 42          |
|    time_elapsed         | 224         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.009343311 |
|    clip_fraction        | 0.0764      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.253      |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00042     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00477    |
|    value_loss           | 0.00263     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30.2         |
|    ep_rew_mean          | 0.973        |
| time/                   |              |
|    fps                  | 384          |
|    iterations           | 43           |
|    time_elapsed         | 228          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0030860337 |
|    clip_fraction        | 0.0519       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.22        |
|    explained_variance   | 0.601        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00874     |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 0.00258      |
------------------------------------------

Evaluation at step 90000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.4        |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 385         |
|    iterations           | 44          |
|    time_elapsed         | 233         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.003355809 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.206      |
|    explained_variance   | 0.614       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00339    |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00251    |
|    value_loss           | 0.00244     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 29.4         |
|    ep_rew_mean          | 0.974        |
| time/                   |              |
|    fps                  | 386          |
|    iterations           | 45           |
|    time_elapsed         | 238          |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.0030894612 |
|    clip_fraction        | 0.0345       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.175       |
|    explained_variance   | 0.624        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00341     |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.0011      |
|    value_loss           | 0.00229      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 29           |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 388          |
|    iterations           | 46           |
|    time_elapsed         | 242          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0029424604 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.163       |
|    explained_variance   | 0.616        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00435     |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.000927    |
|    value_loss           | 0.00217      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.7         |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 389          |
|    iterations           | 47           |
|    time_elapsed         | 247          |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 0.0046655573 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.128       |
|    explained_variance   | 0.582        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00122      |
|    n_updates            | 460          |
|    policy_gradient_loss | 0.000593     |
|    value_loss           | 0.00209      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.8        |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 390         |
|    iterations           | 48          |
|    time_elapsed         | 251         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.022997739 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0863     |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00517     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00186    |
|    value_loss           | 0.00191     |
-----------------------------------------

Evaluation at step 100000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.8        |
|    ep_rew_mean          | 0.975       |
| time/                   |             |
|    fps                  | 390         |
|    iterations           | 49          |
|    time_elapsed         | 256         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.005518333 |
|    clip_fraction        | 0.0388      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00402    |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00589    |
|    value_loss           | 0.00291     |
-----------------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 625      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 2           |
|    time_elapsed         | 7           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.013942268 |
|    clip_fraction        | 0.0522      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -6.72       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0396     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00415    |
|    value_loss           | 2.94e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 461         |
|    iterations           | 3           |
|    time_elapsed         | 13          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.010321014 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | -5.09       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0134      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00311    |
|    value_loss           | 2.88e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 458         |
|    iterations           | 4           |
|    time_elapsed         | 17          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.014878904 |
|    clip_fraction        | 0.0994      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | -2.98       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0192     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0099     |
|    value_loss           | 3.94e-06    |
-----------------------------------------

Evaluation at step 10000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 304         |
|    iterations           | 5           |
|    time_elapsed         | 33          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.014374834 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | -5.85       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.027      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00659    |
|    value_loss           | 6.63e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 320         |
|    iterations           | 6           |
|    time_elapsed         | 38          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.009271673 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -2.66       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0265     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00449    |
|    value_loss           | 7.77e-06    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 0          |
| time/                   |            |
|    fps                  | 335        |
|    iterations           | 7          |
|    time_elapsed         | 42         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.00979677 |
|    clip_fraction        | 0.0647     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.92      |
|    explained_variance   | -3.87      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00936   |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.00696   |
|    value_loss           | 1.52e-05   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 0          |
| time/                   |            |
|    fps                  | 347        |
|    iterations           | 8          |
|    time_elapsed         | 47         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.02369181 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.9       |
|    explained_variance   | -2.29      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0521    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 1.54e-05   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 355         |
|    iterations           | 9           |
|    time_elapsed         | 51          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.008741308 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | -5.1        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0239     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00816    |
|    value_loss           | 2.62e-05    |
-----------------------------------------

Evaluation at step 20000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 289         |
|    iterations           | 10          |
|    time_elapsed         | 70          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.011672192 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | -6.2        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0391     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00931    |
|    value_loss           | 2.3e-05     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 11          |
|    time_elapsed         | 75          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.012764923 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | -3.27       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00679    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00888    |
|    value_loss           | 2.1e-05     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 307         |
|    iterations           | 12          |
|    time_elapsed         | 79          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.016086608 |
|    clip_fraction        | 0.0761      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | -3.75       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00317    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 1.1e-05     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 0.0208      |
| time/                   |             |
|    fps                  | 314         |
|    iterations           | 13          |
|    time_elapsed         | 84          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.011803655 |
|    clip_fraction        | 0.0935      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | -4.16       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00442    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00739    |
|    value_loss           | 3.06e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 977         |
|    ep_rew_mean          | 0.0519      |
| time/                   |             |
|    fps                  | 321         |
|    iterations           | 14          |
|    time_elapsed         | 89          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.010976986 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | -0.0349     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00134     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00371    |
|    value_loss           | 0.0013      |
-----------------------------------------

Evaluation at step 30000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 980         |
|    ep_rew_mean          | 0.0485      |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 15          |
|    time_elapsed         | 107         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.015737608 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | -0.000639   |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00752    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00717    |
|    value_loss           | 0.00191     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 982         |
|    ep_rew_mean          | 0.0456      |
| time/                   |             |
|    fps                  | 292         |
|    iterations           | 16          |
|    time_elapsed         | 111         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.016495302 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | -8.35       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0411     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0097     |
|    value_loss           | 2.83e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 975         |
|    ep_rew_mean          | 0.0543      |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 17          |
|    time_elapsed         | 116         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.012632119 |
|    clip_fraction        | 0.0873      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | -6.63       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00964     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 1.85e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 978         |
|    ep_rew_mean          | 0.0514      |
| time/                   |             |
|    fps                  | 306         |
|    iterations           | 18          |
|    time_elapsed         | 120         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.010365659 |
|    clip_fraction        | 0.0882      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | -0.0254     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.016      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00472    |
|    value_loss           | 0.0007      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 980         |
|    ep_rew_mean          | 0.0487      |
| time/                   |             |
|    fps                  | 312         |
|    iterations           | 19          |
|    time_elapsed         | 124         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.019264068 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | -2.53       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0513     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 4.27e-05    |
-----------------------------------------

Evaluation at step 40000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 982         |
|    ep_rew_mean          | 0.0464      |
| time/                   |             |
|    fps                  | 292         |
|    iterations           | 20          |
|    time_elapsed         | 140         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.014078308 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | -2.22       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0255     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 2.57e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 984         |
|    ep_rew_mean          | 0.0442      |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 21          |
|    time_elapsed         | 144         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.012218559 |
|    clip_fraction        | 0.0865      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | -8.87       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0263     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00984    |
|    value_loss           | 2.83e-05    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 986        |
|    ep_rew_mean          | 0.0422     |
| time/                   |            |
|    fps                  | 303        |
|    iterations           | 22         |
|    time_elapsed         | 148        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.02381035 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.91      |
|    explained_variance   | -7.96      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0268    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0117    |
|    value_loss           | 1.35e-05   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 975         |
|    ep_rew_mean          | 0.0532      |
| time/                   |             |
|    fps                  | 307         |
|    iterations           | 23          |
|    time_elapsed         | 153         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.013348348 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -4.52       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.047      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 1.82e-05    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 977        |
|    ep_rew_mean          | 0.0511     |
| time/                   |            |
|    fps                  | 312        |
|    iterations           | 24         |
|    time_elapsed         | 157        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.00816731 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.92      |
|    explained_variance   | -0.0106    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0283    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 0.00168    |
----------------------------------------

Evaluation at step 50000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 979         |
|    ep_rew_mean          | 0.0491      |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 25          |
|    time_elapsed         | 173         |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.021292698 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | -5.29       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0151     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 6.67e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 981         |
|    ep_rew_mean          | 0.0473      |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 26          |
|    time_elapsed         | 177         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.012004146 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | -1.46       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00948    |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 2.73e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 982         |
|    ep_rew_mean          | 0.0456      |
| time/                   |             |
|    fps                  | 303         |
|    iterations           | 27          |
|    time_elapsed         | 182         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.017420817 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | -1.75       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0231     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 2.46e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 980         |
|    ep_rew_mean          | 0.0492      |
| time/                   |             |
|    fps                  | 307         |
|    iterations           | 28          |
|    time_elapsed         | 186         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.018035231 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | -3.9        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0187      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 1.64e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 973         |
|    ep_rew_mean          | 0.0559      |
| time/                   |             |
|    fps                  | 311         |
|    iterations           | 29          |
|    time_elapsed         | 190         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.009286435 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | -0.0387     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0273     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00757    |
|    value_loss           | 0.00032     |
-----------------------------------------

Evaluation at step 60000: mean reward = 0.00
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 969       |
|    ep_rew_mean          | 0.0622    |
| time/                   |           |
|    fps                  | 297       |
|    iterations           | 30        |
|    time_elapsed         | 206       |
|    total_timesteps      | 61440     |
| train/                  |           |
|    approx_kl            | 0.0145644 |
|    clip_fraction        | 0.133     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.84     |
|    explained_variance   | 0.0318    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00843   |
|    n_updates            | 290       |
|    policy_gradient_loss | -0.00838  |
|    value_loss           | 0.00134   |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 971         |
|    ep_rew_mean          | 0.0603      |
| time/                   |             |
|    fps                  | 301         |
|    iterations           | 31          |
|    time_elapsed         | 210         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.014983125 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | -0.113      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0143     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00862    |
|    value_loss           | 0.000549    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 965         |
|    ep_rew_mean          | 0.0668      |
| time/                   |             |
|    fps                  | 304         |
|    iterations           | 32          |
|    time_elapsed         | 215         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.014054566 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -2.79       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0379     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 4.84e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 967         |
|    ep_rew_mean          | 0.0649      |
| time/                   |             |
|    fps                  | 307         |
|    iterations           | 33          |
|    time_elapsed         | 219         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.012332564 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.00664     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.012       |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00627    |
|    value_loss           | 0.00129     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 968         |
|    ep_rew_mean          | 0.0631      |
| time/                   |             |
|    fps                  | 311         |
|    iterations           | 34          |
|    time_elapsed         | 223         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.016825235 |
|    clip_fraction        | 0.0958      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -4.61       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0123     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 3.78e-05    |
-----------------------------------------

Evaluation at step 70000: mean reward = 0.00
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 970        |
|    ep_rew_mean          | 0.0614     |
| time/                   |            |
|    fps                  | 299        |
|    iterations           | 35         |
|    time_elapsed         | 239        |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.02118942 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.7       |
|    explained_variance   | -2.38      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0305    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0136    |
|    value_loss           | 2.75e-05   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 971         |
|    ep_rew_mean          | 0.0597      |
| time/                   |             |
|    fps                  | 302         |
|    iterations           | 36          |
|    time_elapsed         | 243         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.019560304 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -4.87       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0392     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 3.75e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 968         |
|    ep_rew_mean          | 0.0634      |
| time/                   |             |
|    fps                  | 305         |
|    iterations           | 37          |
|    time_elapsed         | 247         |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.029843368 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | -3.25       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0142      |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 3.66e-05    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 963        |
|    ep_rew_mean          | 0.0685     |
| time/                   |            |
|    fps                  | 308        |
|    iterations           | 38         |
|    time_elapsed         | 252        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.01260153 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.69      |
|    explained_variance   | -0.113     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0273    |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.00677   |
|    value_loss           | 0.000985   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 960         |
|    ep_rew_mean          | 0.073       |
| time/                   |             |
|    fps                  | 311         |
|    iterations           | 39          |
|    time_elapsed         | 256         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.009449856 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | -0.144      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0339     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00558    |
|    value_loss           | 0.00143     |
-----------------------------------------

Evaluation at step 80000: mean reward = 0.00
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 940        |
|    ep_rew_mean          | 0.0942     |
| time/                   |            |
|    fps                  | 301        |
|    iterations           | 40         |
|    time_elapsed         | 271        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.01942125 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.63      |
|    explained_variance   | -0.059     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0286     |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.014     |
|    value_loss           | 0.00106    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 863        |
|    ep_rew_mean          | 0.171      |
| time/                   |            |
|    fps                  | 304        |
|    iterations           | 41         |
|    time_elapsed         | 276        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.01396544 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.6       |
|    explained_variance   | 0.0327     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0048     |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.00663   |
|    value_loss           | 0.00534    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 716         |
|    ep_rew_mean          | 0.316       |
| time/                   |             |
|    fps                  | 306         |
|    iterations           | 42          |
|    time_elapsed         | 280         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.013284026 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.0698      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000586    |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00784    |
|    value_loss           | 0.0226      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 529         |
|    ep_rew_mean          | 0.498       |
| time/                   |             |
|    fps                  | 309         |
|    iterations           | 43          |
|    time_elapsed         | 284         |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.015321008 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.183       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0121      |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.0317      |
-----------------------------------------

Evaluation at step 90000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 332         |
|    ep_rew_mean          | 0.691       |
| time/                   |             |
|    fps                  | 300         |
|    iterations           | 44          |
|    time_elapsed         | 300         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.014711447 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.244       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0255     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.03        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 93.8       |
|    ep_rew_mean          | 0.918      |
| time/                   |            |
|    fps                  | 302        |
|    iterations           | 45         |
|    time_elapsed         | 304        |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.01856194 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.0766     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0218    |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0145    |
|    value_loss           | 0.0236     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 73.1        |
|    ep_rew_mean          | 0.936       |
| time/                   |             |
|    fps                  | 304         |
|    iterations           | 46          |
|    time_elapsed         | 308         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.011374593 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.294       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0135     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.0196      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 58.1        |
|    ep_rew_mean          | 0.949       |
| time/                   |             |
|    fps                  | 307         |
|    iterations           | 47          |
|    time_elapsed         | 313         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.012611087 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0265     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.0135      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50.1         |
|    ep_rew_mean          | 0.956        |
| time/                   |              |
|    fps                  | 309          |
|    iterations           | 48           |
|    time_elapsed         | 317          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0071420223 |
|    clip_fraction        | 0.109        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.44         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00319     |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00785     |
|    value_loss           | 0.0103       |
------------------------------------------

Evaluation at step 100000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 42.7        |
|    ep_rew_mean          | 0.962       |
| time/                   |             |
|    fps                  | 311         |
|    iterations           | 49          |
|    time_elapsed         | 322         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.011063654 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.96       |
|    explained_variance   | 0.494       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00236     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.00808     |
-----------------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 666      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 557         |
|    iterations           | 2           |
|    time_elapsed         | 7           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.013440265 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -1.37       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0188     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00595    |
|    value_loss           | 7.51e-06    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.02e+03     |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 3            |
|    time_elapsed         | 11           |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0096875895 |
|    clip_fraction        | 0.0603       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.92        |
|    explained_variance   | -2.59        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0249      |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00596     |
|    value_loss           | 7.28e-06     |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 956        |
|    ep_rew_mean          | 0.0722     |
| time/                   |            |
|    fps                  | 514        |
|    iterations           | 4          |
|    time_elapsed         | 15         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.01060467 |
|    clip_fraction        | 0.0665     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.92      |
|    explained_variance   | -5.49      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00294   |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.00717   |
|    value_loss           | 5.54e-06   |
----------------------------------------

Evaluation at step 10000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 970         |
|    ep_rew_mean          | 0.0577      |
| time/                   |             |
|    fps                  | 330         |
|    iterations           | 5           |
|    time_elapsed         | 30          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.007912837 |
|    clip_fraction        | 0.0608      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | -0.00742    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0186     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00325    |
|    value_loss           | 0.00137     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 979        |
|    ep_rew_mean          | 0.0481     |
| time/                   |            |
|    fps                  | 348        |
|    iterations           | 6          |
|    time_elapsed         | 35         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.00877147 |
|    clip_fraction        | 0.0559     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | -1.51      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0111    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.00558   |
|    value_loss           | 3.49e-05   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 985         |
|    ep_rew_mean          | 0.0412      |
| time/                   |             |
|    fps                  | 361         |
|    iterations           | 7           |
|    time_elapsed         | 39          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.011465082 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | -2.37       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0627     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 2.44e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 990         |
|    ep_rew_mean          | 0.0361      |
| time/                   |             |
|    fps                  | 373         |
|    iterations           | 8           |
|    time_elapsed         | 43          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.013773838 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | -0.587      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0243     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 1.67e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 994         |
|    ep_rew_mean          | 0.0321      |
| time/                   |             |
|    fps                  | 382         |
|    iterations           | 9           |
|    time_elapsed         | 48          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.014591638 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | -2.02       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.033      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 3.1e-05     |
-----------------------------------------

Evaluation at step 20000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 997         |
|    ep_rew_mean          | 0.0289      |
| time/                   |             |
|    fps                  | 319         |
|    iterations           | 10          |
|    time_elapsed         | 64          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.013256428 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | -1.62       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0286     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 3.66e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 999         |
|    ep_rew_mean          | 0.0262      |
| time/                   |             |
|    fps                  | 329         |
|    iterations           | 11          |
|    time_elapsed         | 68          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.012918105 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -1.47       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0054     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 1.12e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 0.0241      |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 12          |
|    time_elapsed         | 72          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.009711041 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -0.316      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0158     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00982    |
|    value_loss           | 2.83e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 0.0222      |
| time/                   |             |
|    fps                  | 345         |
|    iterations           | 13          |
|    time_elapsed         | 77          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.012870932 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -7.18       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0182      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00721    |
|    value_loss           | 5.69e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 0.0206      |
| time/                   |             |
|    fps                  | 351         |
|    iterations           | 14          |
|    time_elapsed         | 81          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.012384787 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | -7.92       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0128     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00557    |
|    value_loss           | 4.51e-05    |
-----------------------------------------

Evaluation at step 30000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0192      |
| time/                   |             |
|    fps                  | 315         |
|    iterations           | 15          |
|    time_elapsed         | 97          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.011089198 |
|    clip_fraction        | 0.0973      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | -2.89       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0514     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 2.13e-05    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 0.018      |
| time/                   |            |
|    fps                  | 321        |
|    iterations           | 16         |
|    time_elapsed         | 101        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.01058619 |
|    clip_fraction        | 0.0699     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.66      |
|    explained_variance   | -7.43      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0505    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0147    |
|    value_loss           | 1.3e-05    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.017       |
| time/                   |             |
|    fps                  | 328         |
|    iterations           | 17          |
|    time_elapsed         | 106         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.012795314 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | -10.2       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0106     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 2.08e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.016       |
| time/                   |             |
|    fps                  | 333         |
|    iterations           | 18          |
|    time_elapsed         | 110         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.011022507 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | -3.44       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.032      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 1.95e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0152      |
| time/                   |             |
|    fps                  | 339         |
|    iterations           | 19          |
|    time_elapsed         | 114         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.010180519 |
|    clip_fraction        | 0.0634      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | -4.03       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0213     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00454    |
|    value_loss           | 1.41e-05    |
-----------------------------------------

Evaluation at step 40000: mean reward = 0.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 995          |
|    ep_rew_mean          | 0.0303       |
| time/                   |              |
|    fps                  | 314          |
|    iterations           | 20           |
|    time_elapsed         | 130          |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0096405465 |
|    clip_fraction        | 0.0956       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | -7.44        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0394      |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.0115      |
|    value_loss           | 2.4e-05      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 962         |
|    ep_rew_mean          | 0.0656      |
| time/                   |             |
|    fps                  | 319         |
|    iterations           | 21          |
|    time_elapsed         | 134         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.009067285 |
|    clip_fraction        | 0.0771      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.00918     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0171     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00672    |
|    value_loss           | 0.00177     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 936        |
|    ep_rew_mean          | 0.0944     |
| time/                   |            |
|    fps                  | 324        |
|    iterations           | 22         |
|    time_elapsed         | 138        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.01257365 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.6       |
|    explained_variance   | 0.0687     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0243    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0077    |
|    value_loss           | 0.00349    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 781         |
|    ep_rew_mean          | 0.247       |
| time/                   |             |
|    fps                  | 328         |
|    iterations           | 23          |
|    time_elapsed         | 143         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.018269442 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.0891      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0221     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.00299     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 637        |
|    ep_rew_mean          | 0.388      |
| time/                   |            |
|    fps                  | 333        |
|    iterations           | 24         |
|    time_elapsed         | 147        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.01056795 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.5       |
|    explained_variance   | 0.147      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0275     |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.00781   |
|    value_loss           | 0.0222     |
----------------------------------------

Evaluation at step 50000: mean reward = 0.00
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 415        |
|    ep_rew_mean          | 0.604      |
| time/                   |            |
|    fps                  | 314        |
|    iterations           | 25         |
|    time_elapsed         | 162        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.01758572 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.279      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0133    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0114    |
|    value_loss           | 0.0282     |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 220          |
|    ep_rew_mean          | 0.797        |
| time/                   |              |
|    fps                  | 319          |
|    iterations           | 26           |
|    time_elapsed         | 166          |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0055976873 |
|    clip_fraction        | 0.0486       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.297        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00409     |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00394     |
|    value_loss           | 0.031        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 78.8        |
|    ep_rew_mean          | 0.931       |
| time/                   |             |
|    fps                  | 323         |
|    iterations           | 27          |
|    time_elapsed         | 171         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.006964969 |
|    clip_fraction        | 0.0892      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.386       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00238    |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00885    |
|    value_loss           | 0.0205      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 69.2        |
|    ep_rew_mean          | 0.939       |
| time/                   |             |
|    fps                  | 327         |
|    iterations           | 28          |
|    time_elapsed         | 175         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.005629815 |
|    clip_fraction        | 0.0683      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00547     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 0.0187      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 53.9        |
|    ep_rew_mean          | 0.953       |
| time/                   |             |
|    fps                  | 330         |
|    iterations           | 29          |
|    time_elapsed         | 179         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.009928649 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.512       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0118     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.0183      |
-----------------------------------------

Evaluation at step 60000: mean reward = 0.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 44.5         |
|    ep_rew_mean          | 0.961        |
| time/                   |              |
|    fps                  | 315          |
|    iterations           | 30           |
|    time_elapsed         | 194          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0064148013 |
|    clip_fraction        | 0.0624       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.935       |
|    explained_variance   | 0.504        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0168      |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00547     |
|    value_loss           | 0.0141       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 41.1         |
|    ep_rew_mean          | 0.964        |
| time/                   |              |
|    fps                  | 319          |
|    iterations           | 31           |
|    time_elapsed         | 198          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0099532455 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.86        |
|    explained_variance   | 0.487        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00628     |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.0089      |
|    value_loss           | 0.0111       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 39.9        |
|    ep_rew_mean          | 0.965       |
| time/                   |             |
|    fps                  | 322         |
|    iterations           | 32          |
|    time_elapsed         | 203         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.008110509 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.835      |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0243     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.00709     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.6        |
|    ep_rew_mean          | 0.968       |
| time/                   |             |
|    fps                  | 325         |
|    iterations           | 33          |
|    time_elapsed         | 207         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.005499022 |
|    clip_fraction        | 0.0691      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.736      |
|    explained_variance   | 0.456       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000521    |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00594    |
|    value_loss           | 0.00711     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 34.1        |
|    ep_rew_mean          | 0.97        |
| time/                   |             |
|    fps                  | 328         |
|    iterations           | 34          |
|    time_elapsed         | 211         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.009724114 |
|    clip_fraction        | 0.09        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.676      |
|    explained_variance   | 0.539       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00483    |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 0.00465     |
-----------------------------------------

Evaluation at step 70000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 33.8        |
|    ep_rew_mean          | 0.97        |
| time/                   |             |
|    fps                  | 331         |
|    iterations           | 35          |
|    time_elapsed         | 216         |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.009035448 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.574       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00828     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00427    |
|    value_loss           | 0.00439     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.2        |
|    ep_rew_mean          | 0.972       |
| time/                   |             |
|    fps                  | 334         |
|    iterations           | 36          |
|    time_elapsed         | 220         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.005253614 |
|    clip_fraction        | 0.07        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.565      |
|    explained_variance   | 0.55        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00292     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00501    |
|    value_loss           | 0.00389     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30.5         |
|    ep_rew_mean          | 0.973        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 37           |
|    time_elapsed         | 224          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0050649643 |
|    clip_fraction        | 0.0691       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.458       |
|    explained_variance   | 0.57         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00112     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00276     |
|    value_loss           | 0.00357      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 29.1         |
|    ep_rew_mean          | 0.974        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 38           |
|    time_elapsed         | 228          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0053150193 |
|    clip_fraction        | 0.0521       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.353       |
|    explained_variance   | 0.583        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.24e-05     |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00355     |
|    value_loss           | 0.0032       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 43          |
|    ep_rew_mean          | 0.962       |
| time/                   |             |
|    fps                  | 342         |
|    iterations           | 39          |
|    time_elapsed         | 233         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.010296086 |
|    clip_fraction        | 0.0581      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.322      |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00634    |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.000759   |
|    value_loss           | 0.00277     |
-----------------------------------------

Evaluation at step 80000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 52.9        |
|    ep_rew_mean          | 0.954       |
| time/                   |             |
|    fps                  | 344         |
|    iterations           | 40          |
|    time_elapsed         | 237         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.019040678 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.142       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0366     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.0065      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 46.7        |
|    ep_rew_mean          | 0.959       |
| time/                   |             |
|    fps                  | 346         |
|    iterations           | 41          |
|    time_elapsed         | 242         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.011030812 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.493      |
|    explained_variance   | 0.217       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00186    |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.00924     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 37.8       |
|    ep_rew_mean          | 0.967      |
| time/                   |            |
|    fps                  | 349        |
|    iterations           | 42         |
|    time_elapsed         | 246        |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.05022735 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.514     |
|    explained_variance   | 0.415      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0216    |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0133    |
|    value_loss           | 0.00685    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 37.6       |
|    ep_rew_mean          | 0.967      |
| time/                   |            |
|    fps                  | 351        |
|    iterations           | 43         |
|    time_elapsed         | 250        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.00789978 |
|    clip_fraction        | 0.0887     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.61      |
|    explained_variance   | 0.489      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00994    |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.000998  |
|    value_loss           | 0.00562    |
----------------------------------------

Evaluation at step 90000: mean reward = 0.98
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 33.4       |
|    ep_rew_mean          | 0.971      |
| time/                   |            |
|    fps                  | 353        |
|    iterations           | 44         |
|    time_elapsed         | 255        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.01462562 |
|    clip_fraction        | 0.0812     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.467     |
|    explained_variance   | 0.531      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0167     |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.00637   |
|    value_loss           | 0.00484    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30.8        |
|    ep_rew_mean          | 0.973       |
| time/                   |             |
|    fps                  | 355         |
|    iterations           | 45          |
|    time_elapsed         | 259         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.005605571 |
|    clip_fraction        | 0.0642      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.363      |
|    explained_variance   | 0.564       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.005       |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00458    |
|    value_loss           | 0.00435     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30.6         |
|    ep_rew_mean          | 0.973        |
| time/                   |              |
|    fps                  | 357          |
|    iterations           | 46           |
|    time_elapsed         | 263          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0033974287 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.277       |
|    explained_variance   | 0.576        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00755     |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.000725    |
|    value_loss           | 0.00327      |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 36.9      |
|    ep_rew_mean          | 0.968     |
| time/                   |           |
|    fps                  | 359       |
|    iterations           | 47        |
|    time_elapsed         | 267       |
|    total_timesteps      | 96256     |
| train/                  |           |
|    approx_kl            | 0.0736266 |
|    clip_fraction        | 0.185     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.371    |
|    explained_variance   | 0.528     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0378   |
|    n_updates            | 460       |
|    policy_gradient_loss | 0.0299    |
|    value_loss           | 0.00292   |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 39.5        |
|    ep_rew_mean          | 0.965       |
| time/                   |             |
|    fps                  | 361         |
|    iterations           | 48          |
|    time_elapsed         | 272         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.039194442 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.409      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0224     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00661    |
|    value_loss           | 0.00466     |
-----------------------------------------

Evaluation at step 100000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 33.8        |
|    ep_rew_mean          | 0.97        |
| time/                   |             |
|    fps                  | 362         |
|    iterations           | 49          |
|    time_elapsed         | 276         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.012758473 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.462      |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0186     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.00338     |
-----------------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 931      |
|    ep_rew_mean     | 0.132    |
| time/              |          |
|    fps             | 673      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 978         |
|    ep_rew_mean          | 0.0659      |
| time/                   |             |
|    fps                  | 559         |
|    iterations           | 2           |
|    time_elapsed         | 7           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.006679514 |
|    clip_fraction        | 0.0267      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -0.0242     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00973     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00341    |
|    value_loss           | 0.000288    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 993         |
|    ep_rew_mean          | 0.0439      |
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 3           |
|    time_elapsed         | 11          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.011830221 |
|    clip_fraction        | 0.0503      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -5.25       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0106     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00329    |
|    value_loss           | 4.08e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 0.0329      |
| time/                   |             |
|    fps                  | 515         |
|    iterations           | 4           |
|    time_elapsed         | 15          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.009047428 |
|    clip_fraction        | 0.0525      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | -3.55       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0178     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00303    |
|    value_loss           | 1.9e-06     |
-----------------------------------------

Evaluation at step 10000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0263      |
| time/                   |             |
|    fps                  | 330         |
|    iterations           | 5           |
|    time_elapsed         | 31          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.012192884 |
|    clip_fraction        | 0.0764      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -9.02       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0489     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00792    |
|    value_loss           | 1.31e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.022       |
| time/                   |             |
|    fps                  | 348         |
|    iterations           | 6           |
|    time_elapsed         | 35          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.010754824 |
|    clip_fraction        | 0.0456      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | -6.58       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0271     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00257    |
|    value_loss           | 8.35e-07    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0188      |
| time/                   |             |
|    fps                  | 362         |
|    iterations           | 7           |
|    time_elapsed         | 39          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.009804136 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -7.25       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0261     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00339    |
|    value_loss           | 2.07e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0165      |
| time/                   |             |
|    fps                  | 373         |
|    iterations           | 8           |
|    time_elapsed         | 43          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.012770051 |
|    clip_fraction        | 0.0493      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -3.18       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0302     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00588    |
|    value_loss           | 2.86e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0146      |
| time/                   |             |
|    fps                  | 383         |
|    iterations           | 9           |
|    time_elapsed         | 48          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.012150666 |
|    clip_fraction        | 0.0386      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | -7.49       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.012      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00464    |
|    value_loss           | 5.91e-06    |
-----------------------------------------

Evaluation at step 20000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 0.0287      |
| time/                   |             |
|    fps                  | 322         |
|    iterations           | 10          |
|    time_elapsed         | 63          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.008753459 |
|    clip_fraction        | 0.0464      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -3.9        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0151     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00531    |
|    value_loss           | 4.21e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 0.0261      |
| time/                   |             |
|    fps                  | 332         |
|    iterations           | 11          |
|    time_elapsed         | 67          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.011121298 |
|    clip_fraction        | 0.0855      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.0114      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0328     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00651    |
|    value_loss           | 0.00041     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0239      |
| time/                   |             |
|    fps                  | 341         |
|    iterations           | 12          |
|    time_elapsed         | 71          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.020318255 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | -0.784      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0459     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 2.41e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0221      |
| time/                   |             |
|    fps                  | 348         |
|    iterations           | 13          |
|    time_elapsed         | 76          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.010283187 |
|    clip_fraction        | 0.0744      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | -4.28       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0352     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00677    |
|    value_loss           | 1.44e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0205      |
| time/                   |             |
|    fps                  | 355         |
|    iterations           | 14          |
|    time_elapsed         | 80          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.021616854 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | -1.48       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0299     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 2.71e-05    |
-----------------------------------------

Evaluation at step 30000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.0191      |
| time/                   |             |
|    fps                  | 319         |
|    iterations           | 15          |
|    time_elapsed         | 96          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.010294754 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | -4.11       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0296     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 1.48e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 0.0303      |
| time/                   |             |
|    fps                  | 326         |
|    iterations           | 16          |
|    time_elapsed         | 100         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.015782181 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -7.69       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0325     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00592    |
|    value_loss           | 1.25e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 965         |
|    ep_rew_mean          | 0.071       |
| time/                   |             |
|    fps                  | 332         |
|    iterations           | 17          |
|    time_elapsed         | 104         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.011767418 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | -0.02       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0301     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0062     |
|    value_loss           | 0.000694    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 918         |
|    ep_rew_mean          | 0.12        |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 18          |
|    time_elapsed         | 109         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.009810628 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.0487      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0263     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00673    |
|    value_loss           | 0.00282     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 885         |
|    ep_rew_mean          | 0.155       |
| time/                   |             |
|    fps                  | 343         |
|    iterations           | 19          |
|    time_elapsed         | 113         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.009033542 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.0962      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00737    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00455    |
|    value_loss           | 0.00446     |
-----------------------------------------

Evaluation at step 40000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 815         |
|    ep_rew_mean          | 0.226       |
| time/                   |             |
|    fps                  | 317         |
|    iterations           | 20          |
|    time_elapsed         | 129         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.015209068 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.116       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0332     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00992    |
|    value_loss           | 0.00429     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 691         |
|    ep_rew_mean          | 0.346       |
| time/                   |             |
|    fps                  | 322         |
|    iterations           | 21          |
|    time_elapsed         | 133         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.015043133 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.148       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0155     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0094     |
|    value_loss           | 0.0106      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 599         |
|    ep_rew_mean          | 0.435       |
| time/                   |             |
|    fps                  | 327         |
|    iterations           | 22          |
|    time_elapsed         | 137         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.009829216 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.152       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0167     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.0226      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 495         |
|    ep_rew_mean          | 0.534       |
| time/                   |             |
|    fps                  | 331         |
|    iterations           | 23          |
|    time_elapsed         | 141         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.014571193 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.205       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0137     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.0232      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 309         |
|    ep_rew_mean          | 0.717       |
| time/                   |             |
|    fps                  | 336         |
|    iterations           | 24          |
|    time_elapsed         | 146         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.013268195 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.229       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0246     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.0288      |
-----------------------------------------

Evaluation at step 50000: mean reward = 0.00
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 104        |
|    ep_rew_mean          | 0.908      |
| time/                   |            |
|    fps                  | 317        |
|    iterations           | 25         |
|    time_elapsed         | 161        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.01560346 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.211      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00295   |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0137    |
|    value_loss           | 0.0265     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 64.6        |
|    ep_rew_mean          | 0.943       |
| time/                   |             |
|    fps                  | 321         |
|    iterations           | 26          |
|    time_elapsed         | 165         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.011984888 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.298       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00333     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00927    |
|    value_loss           | 0.0198      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 48.4        |
|    ep_rew_mean          | 0.957       |
| time/                   |             |
|    fps                  | 325         |
|    iterations           | 27          |
|    time_elapsed         | 170         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.012908783 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0141     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.0139      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 40.9       |
|    ep_rew_mean          | 0.964      |
| time/                   |            |
|    fps                  | 328        |
|    iterations           | 28         |
|    time_elapsed         | 174        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.01020184 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.964     |
|    explained_variance   | 0.394      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00498   |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 0.01       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.5        |
|    ep_rew_mean          | 0.968       |
| time/                   |             |
|    fps                  | 332         |
|    iterations           | 29          |
|    time_elapsed         | 178         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.010200834 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.561       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0219     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.00734     |
-----------------------------------------

Evaluation at step 60000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 38.9        |
|    ep_rew_mean          | 0.966       |
| time/                   |             |
|    fps                  | 335         |
|    iterations           | 30          |
|    time_elapsed         | 183         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.019141749 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.822      |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0207      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00803    |
|    value_loss           | 0.00682     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 39.5        |
|    ep_rew_mean          | 0.965       |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 31          |
|    time_elapsed         | 187         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.006898592 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.869      |
|    explained_variance   | 0.501       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00739    |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00802    |
|    value_loss           | 0.00625     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 35.9        |
|    ep_rew_mean          | 0.968       |
| time/                   |             |
|    fps                  | 341         |
|    iterations           | 32          |
|    time_elapsed         | 191         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.008238548 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.589       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00921    |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.00634     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 33.9        |
|    ep_rew_mean          | 0.97        |
| time/                   |             |
|    fps                  | 344         |
|    iterations           | 33          |
|    time_elapsed         | 196         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.006032842 |
|    clip_fraction        | 0.0691      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.688      |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0097     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00312    |
|    value_loss           | 0.00541     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.5        |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 347         |
|    iterations           | 34          |
|    time_elapsed         | 200         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.005626591 |
|    clip_fraction        | 0.0867      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.556      |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0236     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00736    |
|    value_loss           | 0.00419     |
-----------------------------------------

Evaluation at step 70000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 33.2        |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 349         |
|    iterations           | 35          |
|    time_elapsed         | 205         |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.003887643 |
|    clip_fraction        | 0.0529      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.498      |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0127     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0045     |
|    value_loss           | 0.00477     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30.2         |
|    ep_rew_mean          | 0.973        |
| time/                   |              |
|    fps                  | 352          |
|    iterations           | 36           |
|    time_elapsed         | 209          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0040895375 |
|    clip_fraction        | 0.0432       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.436       |
|    explained_variance   | 0.537        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00443     |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.0025      |
|    value_loss           | 0.00455      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.2        |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 354         |
|    iterations           | 37          |
|    time_elapsed         | 213         |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.013161974 |
|    clip_fraction        | 0.0446      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.343      |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0116      |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00415    |
|    value_loss           | 0.0035      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.7        |
|    ep_rew_mean          | 0.975       |
| time/                   |             |
|    fps                  | 357         |
|    iterations           | 38          |
|    time_elapsed         | 217         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.005578127 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.287      |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00756     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00166    |
|    value_loss           | 0.00314     |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 29.6          |
|    ep_rew_mean          | 0.974         |
| time/                   |               |
|    fps                  | 359           |
|    iterations           | 39            |
|    time_elapsed         | 222           |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.00086403114 |
|    clip_fraction        | 0.0176        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.261        |
|    explained_variance   | 0.595         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000202      |
|    n_updates            | 380           |
|    policy_gradient_loss | -0.0002       |
|    value_loss           | 0.00285       |
-------------------------------------------

Evaluation at step 80000: mean reward = 0.98
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 29.2       |
|    ep_rew_mean          | 0.974      |
| time/                   |            |
|    fps                  | 361        |
|    iterations           | 40         |
|    time_elapsed         | 226        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.01608964 |
|    clip_fraction        | 0.0552     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.244     |
|    explained_variance   | 0.488      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0137    |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.00524   |
|    value_loss           | 0.0036     |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.5         |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 363          |
|    iterations           | 41           |
|    time_elapsed         | 231          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0048912694 |
|    clip_fraction        | 0.0329       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.206       |
|    explained_variance   | 0.598        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00227     |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00273     |
|    value_loss           | 0.00271      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 29.1         |
|    ep_rew_mean          | 0.974        |
| time/                   |              |
|    fps                  | 365          |
|    iterations           | 42           |
|    time_elapsed         | 235          |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.0038393568 |
|    clip_fraction        | 0.0537       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.208       |
|    explained_variance   | 0.591        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00352     |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00315     |
|    value_loss           | 0.00285      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28          |
|    ep_rew_mean          | 0.975       |
| time/                   |             |
|    fps                  | 367         |
|    iterations           | 43          |
|    time_elapsed         | 239         |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.004804344 |
|    clip_fraction        | 0.0311      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.2        |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.15e-06    |
|    n_updates            | 420         |
|    policy_gradient_loss | 0.00261     |
|    value_loss           | 0.00315     |
-----------------------------------------

Evaluation at step 90000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 27.7        |
|    ep_rew_mean          | 0.976       |
| time/                   |             |
|    fps                  | 369         |
|    iterations           | 44          |
|    time_elapsed         | 244         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.011241562 |
|    clip_fraction        | 0.022       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00279    |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00166    |
|    value_loss           | 0.0023      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30.9        |
|    ep_rew_mean          | 0.973       |
| time/                   |             |
|    fps                  | 371         |
|    iterations           | 45          |
|    time_elapsed         | 248         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.005015755 |
|    clip_fraction        | 0.0147      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.614       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00291     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00058    |
|    value_loss           | 0.0022      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 28.3       |
|    ep_rew_mean          | 0.975      |
| time/                   |            |
|    fps                  | 372        |
|    iterations           | 46         |
|    time_elapsed         | 252        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.01779741 |
|    clip_fraction        | 0.037      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.17      |
|    explained_variance   | 0.411      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00584   |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.00387   |
|    value_loss           | 0.00784    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 27.7        |
|    ep_rew_mean          | 0.976       |
| time/                   |             |
|    fps                  | 374         |
|    iterations           | 47          |
|    time_elapsed         | 256         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.007913909 |
|    clip_fraction        | 0.0362      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000234   |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 0.00241     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36          |
|    ep_rew_mean          | 0.968       |
| time/                   |             |
|    fps                  | 376         |
|    iterations           | 48          |
|    time_elapsed         | 261         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.009631179 |
|    clip_fraction        | 0.0353      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00489     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0014     |
|    value_loss           | 0.0022      |
-----------------------------------------

Evaluation at step 100000: mean reward = 0.98
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 34.1       |
|    ep_rew_mean          | 0.97       |
| time/                   |            |
|    fps                  | 377        |
|    iterations           | 49         |
|    time_elapsed         | 265        |
|    total_timesteps      | 100352     |
| train/                  |            |
|    approx_kl            | 0.03896005 |
|    clip_fraction        | 0.0542     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.22      |
|    explained_variance   | 0.279      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0104    |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.00554   |
|    value_loss           | 0.00947    |
----------------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 0.224    |
| time/              |          |
|    fps             | 652      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0.168       |
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 2           |
|    time_elapsed         | 7           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010352923 |
|    clip_fraction        | 0.0236      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -0.0545     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00605    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00266    |
|    value_loss           | 8.46e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0.146       |
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 3           |
|    time_elapsed         | 11          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.010475362 |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | -0.68       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0205     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00797    |
|    value_loss           | 8.7e-06     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 0.131      |
| time/                   |            |
|    fps                  | 507        |
|    iterations           | 4          |
|    time_elapsed         | 16         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.01262432 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.91      |
|    explained_variance   | -1.36      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0134    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.00811   |
|    value_loss           | 2.34e-05   |
----------------------------------------

Evaluation at step 10000: mean reward = 0.00
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 991        |
|    ep_rew_mean          | 0.157      |
| time/                   |            |
|    fps                  | 326        |
|    iterations           | 5          |
|    time_elapsed         | 31         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.01006143 |
|    clip_fraction        | 0.0607     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.89      |
|    explained_variance   | -4.14      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.022     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.00664   |
|    value_loss           | 1.45e-05   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 901         |
|    ep_rew_mean          | 0.241       |
| time/                   |             |
|    fps                  | 344         |
|    iterations           | 6           |
|    time_elapsed         | 35          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.011322027 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | -0.00451    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0437     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.000683    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 870          |
|    ep_rew_mean          | 0.265        |
| time/                   |              |
|    fps                  | 358          |
|    iterations           | 7            |
|    time_elapsed         | 39           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0147613725 |
|    clip_fraction        | 0.171        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.81        |
|    explained_variance   | 0.0394       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00228      |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00929     |
|    value_loss           | 0.00264      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 844         |
|    ep_rew_mean          | 0.284       |
| time/                   |             |
|    fps                  | 370         |
|    iterations           | 8           |
|    time_elapsed         | 44          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.009294456 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.0676      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0156     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00582    |
|    value_loss           | 0.00212     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 765         |
|    ep_rew_mean          | 0.352       |
| time/                   |             |
|    fps                  | 380         |
|    iterations           | 9           |
|    time_elapsed         | 48          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.009872481 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.114       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0186      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0034     |
|    value_loss           | 0.00212     |
-----------------------------------------

Evaluation at step 20000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 719         |
|    ep_rew_mean          | 0.388       |
| time/                   |             |
|    fps                  | 323         |
|    iterations           | 10          |
|    time_elapsed         | 63          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.007200488 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.183       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0174     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00704    |
|    value_loss           | 0.0058      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 665         |
|    ep_rew_mean          | 0.434       |
| time/                   |             |
|    fps                  | 333         |
|    iterations           | 11          |
|    time_elapsed         | 67          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.008001398 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.274       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.016       |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00728    |
|    value_loss           | 0.00598     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 659         |
|    ep_rew_mean          | 0.437       |
| time/                   |             |
|    fps                  | 341         |
|    iterations           | 12          |
|    time_elapsed         | 71          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.010975713 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0202     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0085     |
|    value_loss           | 0.00603     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 590         |
|    ep_rew_mean          | 0.497       |
| time/                   |             |
|    fps                  | 349         |
|    iterations           | 13          |
|    time_elapsed         | 76          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.008448089 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.285       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0125     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00817    |
|    value_loss           | 0.00391     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 521         |
|    ep_rew_mean          | 0.556       |
| time/                   |             |
|    fps                  | 356         |
|    iterations           | 14          |
|    time_elapsed         | 80          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.011635797 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0324     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00757    |
|    value_loss           | 0.0106      |
-----------------------------------------

Evaluation at step 30000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 444         |
|    ep_rew_mean          | 0.621       |
| time/                   |             |
|    fps                  | 361         |
|    iterations           | 15          |
|    time_elapsed         | 85          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.008464658 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00899    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00629    |
|    value_loss           | 0.0148      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 385          |
|    ep_rew_mean          | 0.672        |
| time/                   |              |
|    fps                  | 367          |
|    iterations           | 16           |
|    time_elapsed         | 89           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0125999395 |
|    clip_fraction        | 0.164        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.62        |
|    explained_variance   | 0.389        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0138      |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.0123      |
|    value_loss           | 0.0176       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 286         |
|    ep_rew_mean          | 0.755       |
| time/                   |             |
|    fps                  | 372         |
|    iterations           | 17          |
|    time_elapsed         | 93          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.011762723 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0471     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.0176      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 154         |
|    ep_rew_mean          | 0.869       |
| time/                   |             |
|    fps                  | 377         |
|    iterations           | 18          |
|    time_elapsed         | 97          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.015375685 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0321     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.0161      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 85.9        |
|    ep_rew_mean          | 0.927       |
| time/                   |             |
|    fps                  | 381         |
|    iterations           | 19          |
|    time_elapsed         | 101         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.013164839 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00498    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.0175      |
-----------------------------------------

Evaluation at step 40000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 63.9        |
|    ep_rew_mean          | 0.945       |
| time/                   |             |
|    fps                  | 384         |
|    iterations           | 20          |
|    time_elapsed         | 106         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.014466856 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.586       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00459    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.0172      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 52.3        |
|    ep_rew_mean          | 0.955       |
| time/                   |             |
|    fps                  | 388         |
|    iterations           | 21          |
|    time_elapsed         | 110         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.012070648 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0256     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00803    |
|    value_loss           | 0.0142      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 46.8        |
|    ep_rew_mean          | 0.96        |
| time/                   |             |
|    fps                  | 390         |
|    iterations           | 22          |
|    time_elapsed         | 115         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.016128592 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0121     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00756    |
|    value_loss           | 0.0104      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 42.4         |
|    ep_rew_mean          | 0.964        |
| time/                   |              |
|    fps                  | 393          |
|    iterations           | 23           |
|    time_elapsed         | 119          |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0129755195 |
|    clip_fraction        | 0.185        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.935       |
|    explained_variance   | 0.506        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00728     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.0156      |
|    value_loss           | 0.00873      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 37.4         |
|    ep_rew_mean          | 0.968        |
| time/                   |              |
|    fps                  | 397          |
|    iterations           | 24           |
|    time_elapsed         | 123          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0071295872 |
|    clip_fraction        | 0.0747       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.792       |
|    explained_variance   | 0.473        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00651      |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00611     |
|    value_loss           | 0.00772      |
------------------------------------------

Evaluation at step 50000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 34.4        |
|    ep_rew_mean          | 0.97        |
| time/                   |             |
|    fps                  | 399         |
|    iterations           | 25          |
|    time_elapsed         | 128         |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.007343362 |
|    clip_fraction        | 0.0809      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.551       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00495     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00479    |
|    value_loss           | 0.00468     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 34          |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 401         |
|    iterations           | 26          |
|    time_elapsed         | 132         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.005204184 |
|    clip_fraction        | 0.066       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.634      |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00201     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00357    |
|    value_loss           | 0.00374     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 33.1         |
|    ep_rew_mean          | 0.971        |
| time/                   |              |
|    fps                  | 404          |
|    iterations           | 27           |
|    time_elapsed         | 136          |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0052191704 |
|    clip_fraction        | 0.0633       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.644       |
|    explained_variance   | 0.522        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0813       |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00301     |
|    value_loss           | 0.00385      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.1        |
|    ep_rew_mean          | 0.972       |
| time/                   |             |
|    fps                  | 406         |
|    iterations           | 28          |
|    time_elapsed         | 140         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.007025985 |
|    clip_fraction        | 0.0582      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.54       |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00569    |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00374    |
|    value_loss           | 0.00304     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31          |
|    ep_rew_mean          | 0.973       |
| time/                   |             |
|    fps                  | 409         |
|    iterations           | 29          |
|    time_elapsed         | 145         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.004864369 |
|    clip_fraction        | 0.0668      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.499      |
|    explained_variance   | 0.584       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000687   |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0057     |
|    value_loss           | 0.00299     |
-----------------------------------------

Evaluation at step 60000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 33.4        |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 410         |
|    iterations           | 30          |
|    time_elapsed         | 149         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.008779377 |
|    clip_fraction        | 0.0659      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.431      |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00851    |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00449    |
|    value_loss           | 0.00314     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 35.3         |
|    ep_rew_mean          | 0.97         |
| time/                   |              |
|    fps                  | 412          |
|    iterations           | 31           |
|    time_elapsed         | 153          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0063509773 |
|    clip_fraction        | 0.0725       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.497       |
|    explained_variance   | 0.417        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00811     |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00512     |
|    value_loss           | 0.00523      |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 30.8       |
|    ep_rew_mean          | 0.973      |
| time/                   |            |
|    fps                  | 414        |
|    iterations           | 32         |
|    time_elapsed         | 158        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.01506714 |
|    clip_fraction        | 0.0944     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.384     |
|    explained_variance   | 0.49       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00244   |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.00852   |
|    value_loss           | 0.00454    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.1        |
|    ep_rew_mean          | 0.975       |
| time/                   |             |
|    fps                  | 416         |
|    iterations           | 33          |
|    time_elapsed         | 162         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.003999783 |
|    clip_fraction        | 0.0357      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.337      |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00332    |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.000831   |
|    value_loss           | 0.00327     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.3         |
|    ep_rew_mean          | 0.976        |
| time/                   |              |
|    fps                  | 418          |
|    iterations           | 34           |
|    time_elapsed         | 166          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0063520805 |
|    clip_fraction        | 0.0377       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.225       |
|    explained_variance   | 0.595        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00375      |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.00231     |
|    value_loss           | 0.00284      |
------------------------------------------

Evaluation at step 70000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 27.8        |
|    ep_rew_mean          | 0.976       |
| time/                   |             |
|    fps                  | 419         |
|    iterations           | 35          |
|    time_elapsed         | 170         |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.006231948 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000536   |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00151    |
|    value_loss           | 0.00259     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 27.7         |
|    ep_rew_mean          | 0.976        |
| time/                   |              |
|    fps                  | 421          |
|    iterations           | 36           |
|    time_elapsed         | 175          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0015673102 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0.596        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00351      |
|    n_updates            | 350          |
|    policy_gradient_loss | -2.99e-06    |
|    value_loss           | 0.00232      |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 27.7          |
|    ep_rew_mean          | 0.976         |
| time/                   |               |
|    fps                  | 422           |
|    iterations           | 37            |
|    time_elapsed         | 179           |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.00044173875 |
|    clip_fraction        | 0.00732       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.144        |
|    explained_variance   | 0.607         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00192       |
|    n_updates            | 360           |
|    policy_gradient_loss | 0.00059       |
|    value_loss           | 0.00228       |
-------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 47.6       |
|    ep_rew_mean          | 0.958      |
| time/                   |            |
|    fps                  | 424        |
|    iterations           | 38         |
|    time_elapsed         | 183        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.32526305 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.226     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0162    |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.00318   |
|    value_loss           | 0.00267    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 67.5        |
|    ep_rew_mean          | 0.939       |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 39          |
|    time_elapsed         | 187         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.014137856 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | -14         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0159     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00221    |
|    value_loss           | 0.00176     |
-----------------------------------------

Evaluation at step 80000: mean reward = 0.00
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 84.4       |
|    ep_rew_mean          | 0.925      |
| time/                   |            |
|    fps                  | 404        |
|    iterations           | 40         |
|    time_elapsed         | 202        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.05535933 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | -1.41      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0556    |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0217    |
|    value_loss           | 0.000879   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 99.9        |
|    ep_rew_mean          | 0.912       |
| time/                   |             |
|    fps                  | 406         |
|    iterations           | 41          |
|    time_elapsed         | 206         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.034427438 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.889      |
|    explained_variance   | 0.0017      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0797      |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.00676     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | 0.898       |
| time/                   |             |
|    fps                  | 407         |
|    iterations           | 42          |
|    time_elapsed         | 211         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.039263293 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.856      |
|    explained_variance   | 0.174       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000568    |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.0067      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 127         |
|    ep_rew_mean          | 0.889       |
| time/                   |             |
|    fps                  | 409         |
|    iterations           | 43          |
|    time_elapsed         | 215         |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.009567326 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.864      |
|    explained_variance   | 0.159       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0254     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.0176      |
-----------------------------------------

Evaluation at step 90000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 58.2        |
|    ep_rew_mean          | 0.95        |
| time/                   |             |
|    fps                  | 410         |
|    iterations           | 44          |
|    time_elapsed         | 219         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.012792769 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.788      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0318     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.0165      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 45.1        |
|    ep_rew_mean          | 0.961       |
| time/                   |             |
|    fps                  | 411         |
|    iterations           | 45          |
|    time_elapsed         | 223         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.012888676 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.012      |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.0147      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 34.8        |
|    ep_rew_mean          | 0.97        |
| time/                   |             |
|    fps                  | 412         |
|    iterations           | 46          |
|    time_elapsed         | 228         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.018294025 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.554      |
|    explained_variance   | 0.563       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00767    |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.00948     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.2        |
|    ep_rew_mean          | 0.972       |
| time/                   |             |
|    fps                  | 414         |
|    iterations           | 47          |
|    time_elapsed         | 232         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.006046002 |
|    clip_fraction        | 0.0536      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.449      |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00617     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0026     |
|    value_loss           | 0.00676     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 31.6         |
|    ep_rew_mean          | 0.973        |
| time/                   |              |
|    fps                  | 415          |
|    iterations           | 48           |
|    time_elapsed         | 236          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0051339106 |
|    clip_fraction        | 0.0663       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.4         |
|    explained_variance   | 0.552        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00217      |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00444     |
|    value_loss           | 0.00476      |
------------------------------------------

Evaluation at step 100000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30.4        |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 416         |
|    iterations           | 49          |
|    time_elapsed         | 240         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.008448452 |
|    clip_fraction        | 0.0541      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.395      |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00432    |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.004      |
|    value_loss           | 0.00364     |
-----------------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 972      |
|    ep_rew_mean     | 0.347    |
| time/              |          |
|    fps             | 679      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 998         |
|    ep_rew_mean          | 0.241       |
| time/                   |             |
|    fps                  | 568         |
|    iterations           | 2           |
|    time_elapsed         | 7           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.013223274 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -0.0634     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0166     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00507    |
|    value_loss           | 0.000153    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.01e+03    |
|    ep_rew_mean          | 0.191       |
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 3           |
|    time_elapsed         | 11          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.006911328 |
|    clip_fraction        | 0.0134      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -0.774      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0215     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00266    |
|    value_loss           | 1.24e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 932         |
|    ep_rew_mean          | 0.243       |
| time/                   |             |
|    fps                  | 526         |
|    iterations           | 4           |
|    time_elapsed         | 15          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.009531795 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | -3.64       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0236     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00514    |
|    value_loss           | 1.31e-05    |
-----------------------------------------

Evaluation at step 10000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 891         |
|    ep_rew_mean          | 0.262       |
| time/                   |             |
|    fps                  | 336         |
|    iterations           | 5           |
|    time_elapsed         | 30          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.009784158 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | -0.012      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00899     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00536    |
|    value_loss           | 0.00177     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 817         |
|    ep_rew_mean          | 0.311       |
| time/                   |             |
|    fps                  | 354         |
|    iterations           | 6           |
|    time_elapsed         | 34          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.011283169 |
|    clip_fraction        | 0.0966      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.0592      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0479     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00815    |
|    value_loss           | 0.00217     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 759         |
|    ep_rew_mean          | 0.363       |
| time/                   |             |
|    fps                  | 369         |
|    iterations           | 7           |
|    time_elapsed         | 38          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.008907262 |
|    clip_fraction        | 0.0734      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.0989      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0468     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00611    |
|    value_loss           | 0.00494     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 673         |
|    ep_rew_mean          | 0.437       |
| time/                   |             |
|    fps                  | 380         |
|    iterations           | 8           |
|    time_elapsed         | 43          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.010960735 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.0187      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.011      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0077     |
|    value_loss           | 0.00415     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 610         |
|    ep_rew_mean          | 0.491       |
| time/                   |             |
|    fps                  | 389         |
|    iterations           | 9           |
|    time_elapsed         | 47          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.017147811 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.126       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0413     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.00837     |
-----------------------------------------

Evaluation at step 20000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 511         |
|    ep_rew_mean          | 0.573       |
| time/                   |             |
|    fps                  | 329         |
|    iterations           | 10          |
|    time_elapsed         | 62          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.016080443 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.171       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00377    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00976    |
|    value_loss           | 0.0102      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 416          |
|    ep_rew_mean          | 0.652        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 11           |
|    time_elapsed         | 66           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0061757397 |
|    clip_fraction        | 0.0654       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.169        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00234      |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00382     |
|    value_loss           | 0.0188       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 323         |
|    ep_rew_mean          | 0.73        |
| time/                   |             |
|    fps                  | 348         |
|    iterations           | 12          |
|    time_elapsed         | 70          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.017464556 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0314     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.0229      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 216         |
|    ep_rew_mean          | 0.817       |
| time/                   |             |
|    fps                  | 356         |
|    iterations           | 13          |
|    time_elapsed         | 74          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.012341768 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.416       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0215     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.0224      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 78.8        |
|    ep_rew_mean          | 0.934       |
| time/                   |             |
|    fps                  | 363         |
|    iterations           | 14          |
|    time_elapsed         | 78          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.016450202 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.047      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.0194      |
-----------------------------------------

Evaluation at step 30000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 49.7        |
|    ep_rew_mean          | 0.958       |
| time/                   |             |
|    fps                  | 368         |
|    iterations           | 15          |
|    time_elapsed         | 83          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.016251057 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0081     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.0182      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 41.9       |
|    ep_rew_mean          | 0.964      |
| time/                   |            |
|    fps                  | 374        |
|    iterations           | 16         |
|    time_elapsed         | 87         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.00847222 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.997     |
|    explained_variance   | 0.549      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0188    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.00918   |
|    value_loss           | 0.0125     |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 38.4         |
|    ep_rew_mean          | 0.967        |
| time/                   |              |
|    fps                  | 379          |
|    iterations           | 17           |
|    time_elapsed         | 91           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0068361945 |
|    clip_fraction        | 0.0961       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.904       |
|    explained_variance   | 0.504        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00217      |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00726     |
|    value_loss           | 0.0111       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 34.1        |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 384         |
|    iterations           | 18          |
|    time_elapsed         | 95          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.008477811 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.735      |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00123    |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.00617     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.3        |
|    ep_rew_mean          | 0.972       |
| time/                   |             |
|    fps                  | 388         |
|    iterations           | 19          |
|    time_elapsed         | 100         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.004867277 |
|    clip_fraction        | 0.0811      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.637      |
|    explained_variance   | 0.594       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0035     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00346    |
|    value_loss           | 0.00459     |
-----------------------------------------

Evaluation at step 40000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 38.3        |
|    ep_rew_mean          | 0.967       |
| time/                   |             |
|    fps                  | 391         |
|    iterations           | 20          |
|    time_elapsed         | 104         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.006515211 |
|    clip_fraction        | 0.0707      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.545      |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00639    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00497    |
|    value_loss           | 0.00326     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 44           |
|    ep_rew_mean          | 0.962        |
| time/                   |              |
|    fps                  | 395          |
|    iterations           | 21           |
|    time_elapsed         | 108          |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0069078254 |
|    clip_fraction        | 0.0517       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.669       |
|    explained_variance   | 0.289        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0233      |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00608     |
|    value_loss           | 0.00917      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.2        |
|    ep_rew_mean          | 0.968       |
| time/                   |             |
|    fps                  | 398         |
|    iterations           | 22          |
|    time_elapsed         | 112         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.006737533 |
|    clip_fraction        | 0.0604      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.573      |
|    explained_variance   | 0.418       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00224     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00426    |
|    value_loss           | 0.0074      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 31.3         |
|    ep_rew_mean          | 0.973        |
| time/                   |              |
|    fps                  | 401          |
|    iterations           | 23           |
|    time_elapsed         | 117          |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0055264095 |
|    clip_fraction        | 0.0729       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.495       |
|    explained_variance   | 0.527        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00738     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00554     |
|    value_loss           | 0.00556      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 31.1         |
|    ep_rew_mean          | 0.973        |
| time/                   |              |
|    fps                  | 404          |
|    iterations           | 24           |
|    time_elapsed         | 121          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0033913078 |
|    clip_fraction        | 0.0691       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.43        |
|    explained_variance   | 0.597        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00428     |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00304     |
|    value_loss           | 0.00359      |
------------------------------------------

Evaluation at step 50000: mean reward = 0.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30           |
|    ep_rew_mean          | 0.974        |
| time/                   |              |
|    fps                  | 406          |
|    iterations           | 25           |
|    time_elapsed         | 125          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0030857387 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.428       |
|    explained_variance   | 0.447        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00278      |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00297     |
|    value_loss           | 0.00397      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 33.1        |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 409         |
|    iterations           | 26          |
|    time_elapsed         | 130         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.097557545 |
|    clip_fraction        | 0.0748      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.326      |
|    explained_variance   | 0.533       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0251      |
|    n_updates            | 250         |
|    policy_gradient_loss | 0.000831    |
|    value_loss           | 0.00297     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 35.8       |
|    ep_rew_mean          | 0.969      |
| time/                   |            |
|    fps                  | 411        |
|    iterations           | 27         |
|    time_elapsed         | 134        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.00753955 |
|    clip_fraction        | 0.0866     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.508     |
|    explained_variance   | 0.507      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00822   |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0064    |
|    value_loss           | 0.00313    |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 33.6         |
|    ep_rew_mean          | 0.971        |
| time/                   |              |
|    fps                  | 414          |
|    iterations           | 28           |
|    time_elapsed         | 138          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0045231283 |
|    clip_fraction        | 0.0611       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.461       |
|    explained_variance   | 0.541        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00333      |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00312     |
|    value_loss           | 0.00314      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 34.8         |
|    ep_rew_mean          | 0.97         |
| time/                   |              |
|    fps                  | 416          |
|    iterations           | 29           |
|    time_elapsed         | 142          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0074436287 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.418       |
|    explained_variance   | 0.56         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00417     |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00123     |
|    value_loss           | 0.00311      |
------------------------------------------

Evaluation at step 60000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 34.4        |
|    ep_rew_mean          | 0.97        |
| time/                   |             |
|    fps                  | 417         |
|    iterations           | 30          |
|    time_elapsed         | 147         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.018567808 |
|    clip_fraction        | 0.099       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.436      |
|    explained_variance   | 0.476       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0071      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00434    |
|    value_loss           | 0.0043      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 33.6        |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 419         |
|    iterations           | 31          |
|    time_elapsed         | 151         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.007934442 |
|    clip_fraction        | 0.068       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.446      |
|    explained_variance   | 0.561       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0042     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00487    |
|    value_loss           | 0.00353     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 32.8         |
|    ep_rew_mean          | 0.972        |
| time/                   |              |
|    fps                  | 421          |
|    iterations           | 32           |
|    time_elapsed         | 155          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0060224812 |
|    clip_fraction        | 0.0499       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.434       |
|    explained_variance   | 0.549        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.018        |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.000728    |
|    value_loss           | 0.00354      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32          |
|    ep_rew_mean          | 0.972       |
| time/                   |             |
|    fps                  | 423         |
|    iterations           | 33          |
|    time_elapsed         | 159         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.007064148 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.414      |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0118     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00484    |
|    value_loss           | 0.00303     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.4        |
|    ep_rew_mean          | 0.973       |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 34          |
|    time_elapsed         | 163         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.009755062 |
|    clip_fraction        | 0.068       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.379      |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000181    |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00435    |
|    value_loss           | 0.00307     |
-----------------------------------------

Evaluation at step 70000: mean reward = 0.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30.1         |
|    ep_rew_mean          | 0.974        |
| time/                   |              |
|    fps                  | 426          |
|    iterations           | 35           |
|    time_elapsed         | 168          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0038216773 |
|    clip_fraction        | 0.044        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.307       |
|    explained_variance   | 0.573        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00471      |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00385     |
|    value_loss           | 0.00284      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.7        |
|    ep_rew_mean          | 0.973       |
| time/                   |             |
|    fps                  | 427         |
|    iterations           | 36          |
|    time_elapsed         | 172         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.006840529 |
|    clip_fraction        | 0.043       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.291      |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00988     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 0.00261     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30          |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 429         |
|    iterations           | 37          |
|    time_elapsed         | 176         |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.011290498 |
|    clip_fraction        | 0.0632      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.296      |
|    explained_variance   | 0.488       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00887     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00692    |
|    value_loss           | 0.00407     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.9        |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 430         |
|    iterations           | 38          |
|    time_elapsed         | 180         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.024318011 |
|    clip_fraction        | 0.042       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.251      |
|    explained_variance   | 0.523       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0037     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00204    |
|    value_loss           | 0.00368     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 29.6       |
|    ep_rew_mean          | 0.974      |
| time/                   |            |
|    fps                  | 432        |
|    iterations           | 39         |
|    time_elapsed         | 184        |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.00500297 |
|    clip_fraction        | 0.0563     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.32      |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0157    |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.00357   |
|    value_loss           | 0.00298    |
----------------------------------------

Evaluation at step 80000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.9        |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 432         |
|    iterations           | 40          |
|    time_elapsed         | 189         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.010030776 |
|    clip_fraction        | 0.0508      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.317      |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0104     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00572    |
|    value_loss           | 0.00274     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30.2        |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 434         |
|    iterations           | 41          |
|    time_elapsed         | 193         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.009063605 |
|    clip_fraction        | 0.059       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.281      |
|    explained_variance   | 0.501       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0167      |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00147    |
|    value_loss           | 0.0032      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 30.7       |
|    ep_rew_mean          | 0.973      |
| time/                   |            |
|    fps                  | 435        |
|    iterations           | 42         |
|    time_elapsed         | 197        |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.06703803 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.304     |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0283    |
|    n_updates            | 410        |
|    policy_gradient_loss | 0.00351    |
|    value_loss           | 0.00312    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.2        |
|    ep_rew_mean          | 0.973       |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 43          |
|    time_elapsed         | 201         |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.008474575 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.364      |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0147     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00693    |
|    value_loss           | 0.00253     |
-----------------------------------------

Evaluation at step 90000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.8        |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 437         |
|    iterations           | 44          |
|    time_elapsed         | 205         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.022343488 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.28       |
|    explained_variance   | 0.645       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0168     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00741    |
|    value_loss           | 0.00259     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.5        |
|    ep_rew_mean          | 0.975       |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 45          |
|    time_elapsed         | 210         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.022832025 |
|    clip_fraction        | 0.0662      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.175      |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00604    |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 0.00249     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.5         |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 439          |
|    iterations           | 46           |
|    time_elapsed         | 214          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0032055676 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.178       |
|    explained_variance   | 0.596        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00927     |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00169     |
|    value_loss           | 0.00252      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.6        |
|    ep_rew_mean          | 0.975       |
| time/                   |             |
|    fps                  | 440         |
|    iterations           | 47          |
|    time_elapsed         | 218         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.002946897 |
|    clip_fraction        | 0.0356      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.208      |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000981    |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.002      |
|    value_loss           | 0.00226     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.5         |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 441          |
|    iterations           | 48           |
|    time_elapsed         | 222          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0077561126 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.165       |
|    explained_variance   | 0.607        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000423    |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00307     |
|    value_loss           | 0.00244      |
------------------------------------------

Evaluation at step 100000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.3        |
|    ep_rew_mean          | 0.975       |
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 49          |
|    time_elapsed         | 226         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.004524354 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00117     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00255    |
|    value_loss           | 0.00261     |
-----------------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 0.205    |
| time/              |          |
|    fps             | 683      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.02e+03     |
|    ep_rew_mean          | 0.194        |
| time/                   |              |
|    fps                  | 574          |
|    iterations           | 2            |
|    time_elapsed         | 7            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0068762675 |
|    clip_fraction        | 0.00962      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -1.06        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0301      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00188     |
|    value_loss           | 1.24e-05     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 990         |
|    ep_rew_mean          | 0.204       |
| time/                   |             |
|    fps                  | 545         |
|    iterations           | 3           |
|    time_elapsed         | 11          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.008262782 |
|    clip_fraction        | 0.0729      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | -0.0335     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0129     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00619    |
|    value_loss           | 7.18e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 998         |
|    ep_rew_mean          | 0.177       |
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 4           |
|    time_elapsed         | 15          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.011250381 |
|    clip_fraction        | 0.088       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -0.0142     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0246     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 0.000294    |
-----------------------------------------

Evaluation at step 10000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 944         |
|    ep_rew_mean          | 0.217       |
| time/                   |             |
|    fps                  | 343         |
|    iterations           | 5           |
|    time_elapsed         | 29          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.009248129 |
|    clip_fraction        | 0.0806      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | -0.94       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0262     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 2.87e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 854         |
|    ep_rew_mean          | 0.292       |
| time/                   |             |
|    fps                  | 361         |
|    iterations           | 6           |
|    time_elapsed         | 34          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.008980553 |
|    clip_fraction        | 0.0664      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.00477     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0291     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00688    |
|    value_loss           | 0.00149     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 745         |
|    ep_rew_mean          | 0.385       |
| time/                   |             |
|    fps                  | 375         |
|    iterations           | 7           |
|    time_elapsed         | 38          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.009101189 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.0798      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0223     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00777    |
|    value_loss           | 0.00435     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 652         |
|    ep_rew_mean          | 0.463       |
| time/                   |             |
|    fps                  | 387         |
|    iterations           | 8           |
|    time_elapsed         | 42          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.010035358 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.109       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0324     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00669    |
|    value_loss           | 0.00713     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 510         |
|    ep_rew_mean          | 0.58        |
| time/                   |             |
|    fps                  | 396         |
|    iterations           | 9           |
|    time_elapsed         | 46          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.008560512 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.12        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.014      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0079     |
|    value_loss           | 0.00932     |
-----------------------------------------

Evaluation at step 20000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 353         |
|    ep_rew_mean          | 0.709       |
| time/                   |             |
|    fps                  | 335         |
|    iterations           | 10          |
|    time_elapsed         | 61          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.011283442 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.197       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0236     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.0188      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 281         |
|    ep_rew_mean          | 0.768       |
| time/                   |             |
|    fps                  | 345         |
|    iterations           | 11          |
|    time_elapsed         | 65          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.011001218 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00818    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.0254      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 135         |
|    ep_rew_mean          | 0.889       |
| time/                   |             |
|    fps                  | 354         |
|    iterations           | 12          |
|    time_elapsed         | 69          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.012719685 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.52        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0072      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.0181      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 66.4        |
|    ep_rew_mean          | 0.944       |
| time/                   |             |
|    fps                  | 362         |
|    iterations           | 13          |
|    time_elapsed         | 73          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.016044384 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.524       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0207     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.0183      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 54.5        |
|    ep_rew_mean          | 0.954       |
| time/                   |             |
|    fps                  | 369         |
|    iterations           | 14          |
|    time_elapsed         | 77          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.008736249 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.57        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00296     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00638    |
|    value_loss           | 0.0147      |
-----------------------------------------

Evaluation at step 30000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 51.4        |
|    ep_rew_mean          | 0.956       |
| time/                   |             |
|    fps                  | 373         |
|    iterations           | 15          |
|    time_elapsed         | 82          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.008315217 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.455       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0199     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.0116      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 46.5        |
|    ep_rew_mean          | 0.96        |
| time/                   |             |
|    fps                  | 379         |
|    iterations           | 16          |
|    time_elapsed         | 86          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.011872077 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0136     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00695    |
|    value_loss           | 0.00909     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 39.4        |
|    ep_rew_mean          | 0.966       |
| time/                   |             |
|    fps                  | 384         |
|    iterations           | 17          |
|    time_elapsed         | 90          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.011464424 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.981      |
|    explained_variance   | 0.564       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00204    |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00818    |
|    value_loss           | 0.00842     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.3        |
|    ep_rew_mean          | 0.968       |
| time/                   |             |
|    fps                  | 388         |
|    iterations           | 18          |
|    time_elapsed         | 94          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.006632435 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.841      |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0104      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00757    |
|    value_loss           | 0.00619     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 35.9         |
|    ep_rew_mean          | 0.969        |
| time/                   |              |
|    fps                  | 392          |
|    iterations           | 19           |
|    time_elapsed         | 99           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0070865764 |
|    clip_fraction        | 0.09         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.81        |
|    explained_variance   | 0.556        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0326      |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.0062      |
|    value_loss           | 0.00486      |
------------------------------------------

Evaluation at step 40000: mean reward = 0.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 32.4         |
|    ep_rew_mean          | 0.972        |
| time/                   |              |
|    fps                  | 395          |
|    iterations           | 20           |
|    time_elapsed         | 103          |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0097377645 |
|    clip_fraction        | 0.0947       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.626       |
|    explained_variance   | 0.564        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0128      |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00749     |
|    value_loss           | 0.00479      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30.6         |
|    ep_rew_mean          | 0.974        |
| time/                   |              |
|    fps                  | 399          |
|    iterations           | 21           |
|    time_elapsed         | 107          |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0069632973 |
|    clip_fraction        | 0.0583       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.505       |
|    explained_variance   | 0.578        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0172      |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00281     |
|    value_loss           | 0.0036       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.5        |
|    ep_rew_mean          | 0.975       |
| time/                   |             |
|    fps                  | 402         |
|    iterations           | 22          |
|    time_elapsed         | 111         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.004970855 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.37       |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0124      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00234    |
|    value_loss           | 0.00288     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30.2        |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 406         |
|    iterations           | 23          |
|    time_elapsed         | 116         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.013083136 |
|    clip_fraction        | 0.064       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.353      |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00751    |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00454    |
|    value_loss           | 0.0026      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 29.9         |
|    ep_rew_mean          | 0.974        |
| time/                   |              |
|    fps                  | 409          |
|    iterations           | 24           |
|    time_elapsed         | 120          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0048256577 |
|    clip_fraction        | 0.0719       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.387       |
|    explained_variance   | 0.595        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00575     |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00234     |
|    value_loss           | 0.00281      |
------------------------------------------

Evaluation at step 50000: mean reward = 0.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 29.4         |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 410          |
|    iterations           | 25           |
|    time_elapsed         | 124          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0045719873 |
|    clip_fraction        | 0.057        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.315       |
|    explained_variance   | 0.596        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0112      |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00316     |
|    value_loss           | 0.00281      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.6         |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 413          |
|    iterations           | 26           |
|    time_elapsed         | 128          |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0026207627 |
|    clip_fraction        | 0.0381       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.26        |
|    explained_variance   | 0.584        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0208      |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00188     |
|    value_loss           | 0.00269      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.1        |
|    ep_rew_mean          | 0.976       |
| time/                   |             |
|    fps                  | 415         |
|    iterations           | 27          |
|    time_elapsed         | 132         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.004262789 |
|    clip_fraction        | 0.0608      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.23       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00269    |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00223    |
|    value_loss           | 0.00254     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28           |
|    ep_rew_mean          | 0.976        |
| time/                   |              |
|    fps                  | 418          |
|    iterations           | 28           |
|    time_elapsed         | 137          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0041160397 |
|    clip_fraction        | 0.0404       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.155       |
|    explained_variance   | 0.599        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00537      |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00217     |
|    value_loss           | 0.00238      |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 32.1       |
|    ep_rew_mean          | 0.972      |
| time/                   |            |
|    fps                  | 420        |
|    iterations           | 29         |
|    time_elapsed         | 141        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.06964363 |
|    clip_fraction        | 0.0479     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.223     |
|    explained_variance   | 0.592      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0175    |
|    n_updates            | 280        |
|    policy_gradient_loss | 0.00851    |
|    value_loss           | 0.0024     |
----------------------------------------

Evaluation at step 60000: mean reward = 0.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 34.4         |
|    ep_rew_mean          | 0.97         |
| time/                   |              |
|    fps                  | 421          |
|    iterations           | 30           |
|    time_elapsed         | 145          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0058032125 |
|    clip_fraction        | 0.0659       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.359       |
|    explained_variance   | 0.535        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0152       |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.003       |
|    value_loss           | 0.00388      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 33.7        |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 423         |
|    iterations           | 31          |
|    time_elapsed         | 149         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.014962661 |
|    clip_fraction        | 0.0846      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.354      |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0126     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00307    |
|    value_loss           | 0.00401     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 31.4         |
|    ep_rew_mean          | 0.973        |
| time/                   |              |
|    fps                  | 425          |
|    iterations           | 32           |
|    time_elapsed         | 154          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0031927987 |
|    clip_fraction        | 0.0637       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.322       |
|    explained_variance   | 0.548        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0154       |
|    n_updates            | 310          |
|    policy_gradient_loss | 0.00103      |
|    value_loss           | 0.00353      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30.1         |
|    ep_rew_mean          | 0.974        |
| time/                   |              |
|    fps                  | 427          |
|    iterations           | 33           |
|    time_elapsed         | 158          |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0042723017 |
|    clip_fraction        | 0.0327       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.229       |
|    explained_variance   | 0.582        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00221      |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.000559    |
|    value_loss           | 0.00358      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 29           |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 428          |
|    iterations           | 34           |
|    time_elapsed         | 162          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0022660317 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.197       |
|    explained_variance   | 0.58         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0113       |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.000459    |
|    value_loss           | 0.00344      |
------------------------------------------

Evaluation at step 70000: mean reward = 0.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.2         |
|    ep_rew_mean          | 0.976        |
| time/                   |              |
|    fps                  | 429          |
|    iterations           | 35           |
|    time_elapsed         | 166          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0013041245 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.148       |
|    explained_variance   | 0.597        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00458     |
|    n_updates            | 340          |
|    policy_gradient_loss | 0.0017       |
|    value_loss           | 0.00372      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 27.7         |
|    ep_rew_mean          | 0.976        |
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 36           |
|    time_elapsed         | 171          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0018502884 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0906      |
|    explained_variance   | 0.594        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00399     |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.000638    |
|    value_loss           | 0.00244      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28           |
|    ep_rew_mean          | 0.976        |
| time/                   |              |
|    fps                  | 432          |
|    iterations           | 37           |
|    time_elapsed         | 175          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0017932819 |
|    clip_fraction        | 0.00635      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.102       |
|    explained_variance   | 0.608        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00385     |
|    n_updates            | 360          |
|    policy_gradient_loss | 0.000756     |
|    value_loss           | 0.00242      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.1        |
|    ep_rew_mean          | 0.976       |
| time/                   |             |
|    fps                  | 433         |
|    iterations           | 38          |
|    time_elapsed         | 179         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.013990175 |
|    clip_fraction        | 0.0164      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0061     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.000664   |
|    value_loss           | 0.00233     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 27.7        |
|    ep_rew_mean          | 0.976       |
| time/                   |             |
|    fps                  | 435         |
|    iterations           | 39          |
|    time_elapsed         | 183         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.013167295 |
|    clip_fraction        | 0.0194      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0999     |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0102      |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00167    |
|    value_loss           | 0.00224     |
-----------------------------------------

Evaluation at step 80000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.2        |
|    ep_rew_mean          | 0.968       |
| time/                   |             |
|    fps                  | 435         |
|    iterations           | 40          |
|    time_elapsed         | 187         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.008816479 |
|    clip_fraction        | 0.0233      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0954     |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00245    |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00107    |
|    value_loss           | 0.0021      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 38          |
|    ep_rew_mean          | 0.967       |
| time/                   |             |
|    fps                  | 437         |
|    iterations           | 41          |
|    time_elapsed         | 192         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.015247678 |
|    clip_fraction        | 0.0629      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.186      |
|    explained_variance   | 0.14        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00173     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00516    |
|    value_loss           | 0.00722     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.2        |
|    ep_rew_mean          | 0.976       |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 42          |
|    time_elapsed         | 196         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.016933981 |
|    clip_fraction        | 0.0499      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0121     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 0.00548     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.2         |
|    ep_rew_mean          | 0.976        |
| time/                   |              |
|    fps                  | 439          |
|    iterations           | 43           |
|    time_elapsed         | 200          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0032066249 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.111       |
|    explained_variance   | 0.608        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0072       |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00314     |
|    value_loss           | 0.00298      |
------------------------------------------

Evaluation at step 90000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.4        |
|    ep_rew_mean          | 0.975       |
| time/                   |             |
|    fps                  | 439         |
|    iterations           | 44          |
|    time_elapsed         | 204         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.003911738 |
|    clip_fraction        | 0.034       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.154      |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00115     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.000473   |
|    value_loss           | 0.00262     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28          |
|    ep_rew_mean          | 0.976       |
| time/                   |             |
|    fps                  | 440         |
|    iterations           | 45          |
|    time_elapsed         | 209         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.007052344 |
|    clip_fraction        | 0.0224      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.586       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00392    |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.000677   |
|    value_loss           | 0.00209     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 29.2         |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 441          |
|    iterations           | 46           |
|    time_elapsed         | 213          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0017377231 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.125       |
|    explained_variance   | 0.605        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0105      |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.000876    |
|    value_loss           | 0.0022       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.5         |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 442          |
|    iterations           | 47           |
|    time_elapsed         | 217          |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 0.0123874135 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.151       |
|    explained_variance   | 0.505        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.015       |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00372     |
|    value_loss           | 0.00293      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28           |
|    ep_rew_mean          | 0.976        |
| time/                   |              |
|    fps                  | 443          |
|    iterations           | 48           |
|    time_elapsed         | 221          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0051198285 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.138       |
|    explained_variance   | 0.606        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00187      |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.000592    |
|    value_loss           | 0.00216      |
------------------------------------------

Evaluation at step 100000: mean reward = 0.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.4         |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 444          |
|    iterations           | 49           |
|    time_elapsed         | 225          |
|    total_timesteps      | 100352       |
| train/                  |              |
|    approx_kl            | 0.0031465665 |
|    clip_fraction        | 0.0253       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.133       |
|    explained_variance   | 0.606        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00196     |
|    n_updates            | 480          |
|    policy_gradient_loss | 0.000304     |
|    value_loss           | 0.00229      |
------------------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 685      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0.168       |
| time/                   |             |
|    fps                  | 570         |
|    iterations           | 2           |
|    time_elapsed         | 7           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.012173463 |
|    clip_fraction        | 0.0316      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -4.72       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0157     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00439    |
|    value_loss           | 4.66e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0.138       |
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 3           |
|    time_elapsed         | 11          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.008912687 |
|    clip_fraction        | 0.00376     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -1.83       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00478    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 2.16e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0.125       |
| time/                   |             |
|    fps                  | 526         |
|    iterations           | 4           |
|    time_elapsed         | 15          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.015452416 |
|    clip_fraction        | 0.0832      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -0.456      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00749    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00748    |
|    value_loss           | 2.58e-06    |
-----------------------------------------

Evaluation at step 10000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 955         |
|    ep_rew_mean          | 0.182       |
| time/                   |             |
|    fps                  | 340         |
|    iterations           | 5           |
|    time_elapsed         | 30          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.004443588 |
|    clip_fraction        | 0.00142     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -2.13       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0195     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.000898   |
|    value_loss           | 2.64e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 967         |
|    ep_rew_mean          | 0.161       |
| time/                   |             |
|    fps                  | 358         |
|    iterations           | 6           |
|    time_elapsed         | 34          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.013349662 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | -0.000226   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0173      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00609    |
|    value_loss           | 0.00208     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 975         |
|    ep_rew_mean          | 0.146       |
| time/                   |             |
|    fps                  | 373         |
|    iterations           | 7           |
|    time_elapsed         | 38          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.009426546 |
|    clip_fraction        | 0.0565      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | -1.84       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0289     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00333    |
|    value_loss           | 1.81e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 981         |
|    ep_rew_mean          | 0.139       |
| time/                   |             |
|    fps                  | 384         |
|    iterations           | 8           |
|    time_elapsed         | 42          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.010345008 |
|    clip_fraction        | 0.085       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | -1.49       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00748     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00649    |
|    value_loss           | 1.55e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 986         |
|    ep_rew_mean          | 0.129       |
| time/                   |             |
|    fps                  | 393         |
|    iterations           | 9           |
|    time_elapsed         | 46          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.013441555 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | -2.67       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.013       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00521    |
|    value_loss           | 1.19e-05    |
-----------------------------------------

Evaluation at step 20000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 990         |
|    ep_rew_mean          | 0.123       |
| time/                   |             |
|    fps                  | 332         |
|    iterations           | 10          |
|    time_elapsed         | 61          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.013158576 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | -1.89       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0275     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 1.18e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 974         |
|    ep_rew_mean          | 0.134       |
| time/                   |             |
|    fps                  | 342         |
|    iterations           | 11          |
|    time_elapsed         | 65          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.010115034 |
|    clip_fraction        | 0.0446      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | -2.59       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0142     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00326    |
|    value_loss           | 1.85e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 978         |
|    ep_rew_mean          | 0.127       |
| time/                   |             |
|    fps                  | 351         |
|    iterations           | 12          |
|    time_elapsed         | 69          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.010171168 |
|    clip_fraction        | 0.0392      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | -0.0254     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0181     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00388    |
|    value_loss           | 0.00105     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 981         |
|    ep_rew_mean          | 0.121       |
| time/                   |             |
|    fps                  | 359         |
|    iterations           | 13          |
|    time_elapsed         | 74          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.011943659 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | -2.7        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0314     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 3.68e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 971         |
|    ep_rew_mean          | 0.13        |
| time/                   |             |
|    fps                  | 366         |
|    iterations           | 14          |
|    time_elapsed         | 78          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.009460194 |
|    clip_fraction        | 0.0826      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | -4.13       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00523     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0081     |
|    value_loss           | 2.02e-05    |
-----------------------------------------

Evaluation at step 30000: mean reward = 0.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 959          |
|    ep_rew_mean          | 0.14         |
| time/                   |              |
|    fps                  | 330          |
|    iterations           | 15           |
|    time_elapsed         | 92           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0131934825 |
|    clip_fraction        | 0.148        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.84        |
|    explained_variance   | -0.00202     |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0241      |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.01        |
|    value_loss           | 0.000768     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 963         |
|    ep_rew_mean          | 0.134       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 16          |
|    time_elapsed         | 97          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.011969843 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | -0.029      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00928    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 0.0012      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 967         |
|    ep_rew_mean          | 0.128       |
| time/                   |             |
|    fps                  | 343         |
|    iterations           | 17          |
|    time_elapsed         | 101         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.022130644 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | -4.09       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0289     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 5.28e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 970         |
|    ep_rew_mean          | 0.123       |
| time/                   |             |
|    fps                  | 348         |
|    iterations           | 18          |
|    time_elapsed         | 105         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.007230697 |
|    clip_fraction        | 0.0358      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | -3.8        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0283     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.003      |
|    value_loss           | 1.9e-05     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 973         |
|    ep_rew_mean          | 0.118       |
| time/                   |             |
|    fps                  | 354         |
|    iterations           | 19          |
|    time_elapsed         | 109         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.009956308 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | -13.4       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0123     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00984    |
|    value_loss           | 1.54e-05    |
-----------------------------------------

Evaluation at step 40000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 971         |
|    ep_rew_mean          | 0.118       |
| time/                   |             |
|    fps                  | 329         |
|    iterations           | 20          |
|    time_elapsed         | 124         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.010960112 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | -4.15       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0284     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 1.26e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 941         |
|    ep_rew_mean          | 0.148       |
| time/                   |             |
|    fps                  | 334         |
|    iterations           | 21          |
|    time_elapsed         | 128         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.010962969 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | -0.114      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0169      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00518    |
|    value_loss           | 0.000387    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 915         |
|    ep_rew_mean          | 0.173       |
| time/                   |             |
|    fps                  | 339         |
|    iterations           | 22          |
|    time_elapsed         | 132         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.012603062 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.0145      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0214     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00846    |
|    value_loss           | 0.00362     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 785         |
|    ep_rew_mean          | 0.294       |
| time/                   |             |
|    fps                  | 344         |
|    iterations           | 23          |
|    time_elapsed         | 136         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.010910626 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | -0.0549     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000293   |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00708    |
|    value_loss           | 0.00439     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 680         |
|    ep_rew_mean          | 0.389       |
| time/                   |             |
|    fps                  | 348         |
|    iterations           | 24          |
|    time_elapsed         | 141         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.010147249 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.0364      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0104      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.0214      |
-----------------------------------------

Evaluation at step 50000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 609         |
|    ep_rew_mean          | 0.454       |
| time/                   |             |
|    fps                  | 329         |
|    iterations           | 25          |
|    time_elapsed         | 155         |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.012159382 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.192       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0135      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.023       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 470         |
|    ep_rew_mean          | 0.576       |
| time/                   |             |
|    fps                  | 333         |
|    iterations           | 26          |
|    time_elapsed         | 159         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.017542334 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.112       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00224    |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.0202      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 261         |
|    ep_rew_mean          | 0.769       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 27          |
|    time_elapsed         | 163         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.014736464 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.21        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0162     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00986    |
|    value_loss           | 0.0273      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 89.7        |
|    ep_rew_mean          | 0.924       |
| time/                   |             |
|    fps                  | 341         |
|    iterations           | 28          |
|    time_elapsed         | 168         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.015106628 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0137      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.027       |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 55.5         |
|    ep_rew_mean          | 0.953        |
| time/                   |              |
|    fps                  | 344          |
|    iterations           | 29           |
|    time_elapsed         | 172          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0124461455 |
|    clip_fraction        | 0.133        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.2         |
|    explained_variance   | 0.244        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00126     |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.0111      |
|    value_loss           | 0.0227       |
------------------------------------------

Evaluation at step 60000: mean reward = 0.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 42.5         |
|    ep_rew_mean          | 0.964        |
| time/                   |              |
|    fps                  | 347          |
|    iterations           | 30           |
|    time_elapsed         | 176          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0090668565 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0.258        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00303     |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.0101      |
|    value_loss           | 0.0164       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.7        |
|    ep_rew_mean          | 0.969       |
| time/                   |             |
|    fps                  | 351         |
|    iterations           | 31          |
|    time_elapsed         | 180         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.007666991 |
|    clip_fraction        | 0.0737      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.877      |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00536     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00515    |
|    value_loss           | 0.0114      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 35          |
|    ep_rew_mean          | 0.97        |
| time/                   |             |
|    fps                  | 354         |
|    iterations           | 32          |
|    time_elapsed         | 184         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.006285076 |
|    clip_fraction        | 0.0879      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.397       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00938    |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0074     |
|    value_loss           | 0.00652     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 34          |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 357         |
|    iterations           | 33          |
|    time_elapsed         | 189         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.004928062 |
|    clip_fraction        | 0.0668      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.672      |
|    explained_variance   | 0.434       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.016      |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 0.00522     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 32.9         |
|    ep_rew_mean          | 0.972        |
| time/                   |              |
|    fps                  | 360          |
|    iterations           | 34           |
|    time_elapsed         | 193          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0054623894 |
|    clip_fraction        | 0.0832       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.607       |
|    explained_variance   | 0.528        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00552     |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.00427     |
|    value_loss           | 0.00326      |
------------------------------------------

Evaluation at step 70000: mean reward = 0.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 33           |
|    ep_rew_mean          | 0.972        |
| time/                   |              |
|    fps                  | 362          |
|    iterations           | 35           |
|    time_elapsed         | 197          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0072273198 |
|    clip_fraction        | 0.0712       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.572       |
|    explained_variance   | 0.587        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00855     |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00317     |
|    value_loss           | 0.00329      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.2        |
|    ep_rew_mean          | 0.972       |
| time/                   |             |
|    fps                  | 365         |
|    iterations           | 36          |
|    time_elapsed         | 201         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.012410674 |
|    clip_fraction        | 0.0723      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.519       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000891    |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 0.00434     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.1        |
|    ep_rew_mean          | 0.973       |
| time/                   |             |
|    fps                  | 367         |
|    iterations           | 37          |
|    time_elapsed         | 206         |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.008583943 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.445      |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00995    |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 0.00342     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 40          |
|    ep_rew_mean          | 0.966       |
| time/                   |             |
|    fps                  | 370         |
|    iterations           | 38          |
|    time_elapsed         | 210         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.076776475 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.433      |
|    explained_variance   | 0.55        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0139     |
|    n_updates            | 370         |
|    policy_gradient_loss | 0.0349      |
|    value_loss           | 0.00343     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 43.6        |
|    ep_rew_mean          | 0.963       |
| time/                   |             |
|    fps                  | 372         |
|    iterations           | 39          |
|    time_elapsed         | 214         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.012264876 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.625      |
|    explained_variance   | 0.303       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0178     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.00732     |
-----------------------------------------

Evaluation at step 80000: mean reward = 0.98
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 35.3      |
|    ep_rew_mean          | 0.97      |
| time/                   |           |
|    fps                  | 374       |
|    iterations           | 40        |
|    time_elapsed         | 218       |
|    total_timesteps      | 81920     |
| train/                  |           |
|    approx_kl            | 0.0088324 |
|    clip_fraction        | 0.138     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.554    |
|    explained_variance   | 0.305     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00817  |
|    n_updates            | 390       |
|    policy_gradient_loss | -0.0122   |
|    value_loss           | 0.0054    |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 34.3         |
|    ep_rew_mean          | 0.97         |
| time/                   |              |
|    fps                  | 376          |
|    iterations           | 41           |
|    time_elapsed         | 222          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0047387183 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.495       |
|    explained_variance   | 0.549        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0165      |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00726     |
|    value_loss           | 0.0047       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 33.1        |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 378         |
|    iterations           | 42          |
|    time_elapsed         | 227         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.010373734 |
|    clip_fraction        | 0.0979      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 0.489       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00457    |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00632    |
|    value_loss           | 0.0045      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.7        |
|    ep_rew_mean          | 0.973       |
| time/                   |             |
|    fps                  | 380         |
|    iterations           | 43          |
|    time_elapsed         | 231         |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.004586265 |
|    clip_fraction        | 0.046       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.481      |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0144     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00226    |
|    value_loss           | 0.00364     |
-----------------------------------------

Evaluation at step 90000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.1        |
|    ep_rew_mean          | 0.973       |
| time/                   |             |
|    fps                  | 382         |
|    iterations           | 44          |
|    time_elapsed         | 235         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.027644906 |
|    clip_fraction        | 0.0435      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.4        |
|    explained_variance   | 0.544       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000785    |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0042     |
|    value_loss           | 0.00414     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30.3        |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 384         |
|    iterations           | 45          |
|    time_elapsed         | 239         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.007780861 |
|    clip_fraction        | 0.0499      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.411      |
|    explained_variance   | 0.585       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00335    |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.000154   |
|    value_loss           | 0.00361     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 29.9         |
|    ep_rew_mean          | 0.974        |
| time/                   |              |
|    fps                  | 386          |
|    iterations           | 46           |
|    time_elapsed         | 244          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0059123067 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.356       |
|    explained_variance   | 0.575        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00122     |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00227     |
|    value_loss           | 0.00315      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.6        |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 387         |
|    iterations           | 47          |
|    time_elapsed         | 248         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.002246506 |
|    clip_fraction        | 0.0246      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.361      |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00495     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.000733   |
|    value_loss           | 0.00299     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.3        |
|    ep_rew_mean          | 0.975       |
| time/                   |             |
|    fps                  | 389         |
|    iterations           | 48          |
|    time_elapsed         | 252         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.015302973 |
|    clip_fraction        | 0.0298      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.393      |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0142     |
|    n_updates            | 470         |
|    policy_gradient_loss | 0.00206     |
|    value_loss           | 0.00318     |
-----------------------------------------

Evaluation at step 100000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.9        |
|    ep_rew_mean          | 0.975       |
| time/                   |             |
|    fps                  | 390         |
|    iterations           | 49          |
|    time_elapsed         | 256         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.003136051 |
|    clip_fraction        | 0.0455      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.26       |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00175    |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00172    |
|    value_loss           | 0.00269     |
-----------------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 780      |
|    ep_rew_mean     | 0.444    |
| time/              |          |
|    fps             | 685      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 902         |
|    ep_rew_mean          | 0.294       |
| time/                   |             |
|    fps                  | 572         |
|    iterations           | 2           |
|    time_elapsed         | 7           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011545965 |
|    clip_fraction        | 0.068       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.0104      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0408     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00786    |
|    value_loss           | 0.0011      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 875          |
|    ep_rew_mean          | 0.301        |
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 3            |
|    time_elapsed         | 11           |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0050988523 |
|    clip_fraction        | 0.0464       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -2.62        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.026       |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00483     |
|    value_loss           | 3.27e-05     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 910         |
|    ep_rew_mean          | 0.264       |
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 4           |
|    time_elapsed         | 15          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.011044799 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.0638      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0331     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00804    |
|    value_loss           | 0.000798    |
-----------------------------------------

Evaluation at step 10000: mean reward = 0.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 933          |
|    ep_rew_mean          | 0.226        |
| time/                   |              |
|    fps                  | 341          |
|    iterations           | 5            |
|    time_elapsed         | 29           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.0069706645 |
|    clip_fraction        | 0.0575       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | -0.117       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0101      |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00922     |
|    value_loss           | 6.98e-05     |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 948        |
|    ep_rew_mean          | 0.199      |
| time/                   |            |
|    fps                  | 359        |
|    iterations           | 6          |
|    time_elapsed         | 34         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.01674396 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.87      |
|    explained_variance   | -2.62      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0311    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0162    |
|    value_loss           | 2.21e-05   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 959         |
|    ep_rew_mean          | 0.178       |
| time/                   |             |
|    fps                  | 373         |
|    iterations           | 7           |
|    time_elapsed         | 38          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.010759266 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | -2.84       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0157     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 1.66e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 967         |
|    ep_rew_mean          | 0.161       |
| time/                   |             |
|    fps                  | 384         |
|    iterations           | 8           |
|    time_elapsed         | 42          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.008271122 |
|    clip_fraction        | 0.0507      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | -6.01       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0351     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00426    |
|    value_loss           | 1.67e-05    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 974        |
|    ep_rew_mean          | 0.148      |
| time/                   |            |
|    fps                  | 393        |
|    iterations           | 9          |
|    time_elapsed         | 46         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.00780729 |
|    clip_fraction        | 0.0688     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.84      |
|    explained_variance   | -3.49      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00374   |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.00618   |
|    value_loss           | 1.06e-05   |
----------------------------------------

Evaluation at step 20000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 979         |
|    ep_rew_mean          | 0.136       |
| time/                   |             |
|    fps                  | 332         |
|    iterations           | 10          |
|    time_elapsed         | 61          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.011698168 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | -5.15       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.001       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00869    |
|    value_loss           | 2.2e-05     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 983        |
|    ep_rew_mean          | 0.127      |
| time/                   |            |
|    fps                  | 342        |
|    iterations           | 11         |
|    time_elapsed         | 65         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.01384301 |
|    clip_fraction        | 0.0993     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.86      |
|    explained_variance   | -2.76      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0261    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0107    |
|    value_loss           | 2.24e-05   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 938         |
|    ep_rew_mean          | 0.165       |
| time/                   |             |
|    fps                  | 350         |
|    iterations           | 12          |
|    time_elapsed         | 70          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.008704607 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | -6.62       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0257     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00638    |
|    value_loss           | 2.42e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 912         |
|    ep_rew_mean          | 0.188       |
| time/                   |             |
|    fps                  | 358         |
|    iterations           | 13          |
|    time_elapsed         | 74          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.009672547 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | -0.0093     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00296     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00779    |
|    value_loss           | 0.00373     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 884         |
|    ep_rew_mean          | 0.213       |
| time/                   |             |
|    fps                  | 365         |
|    iterations           | 14          |
|    time_elapsed         | 78          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.010077238 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.0783      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0217     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00576    |
|    value_loss           | 0.00217     |
-----------------------------------------

Evaluation at step 30000: mean reward = 0.00
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 812          |
|    ep_rew_mean          | 0.28         |
| time/                   |              |
|    fps                  | 330          |
|    iterations           | 15           |
|    time_elapsed         | 92           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0130117675 |
|    clip_fraction        | 0.149        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.149        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0214      |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00803     |
|    value_loss           | 0.00291      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 728         |
|    ep_rew_mean          | 0.357       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 16          |
|    time_elapsed         | 97          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.011579185 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.111       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00296     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00754    |
|    value_loss           | 0.00833     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 654         |
|    ep_rew_mean          | 0.424       |
| time/                   |             |
|    fps                  | 343         |
|    iterations           | 17          |
|    time_elapsed         | 101         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.009425171 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.203       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0105      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00925    |
|    value_loss           | 0.0127      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 559         |
|    ep_rew_mean          | 0.509       |
| time/                   |             |
|    fps                  | 349         |
|    iterations           | 18          |
|    time_elapsed         | 105         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.010412644 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0193     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00625    |
|    value_loss           | 0.0135      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 463         |
|    ep_rew_mean          | 0.594       |
| time/                   |             |
|    fps                  | 354         |
|    iterations           | 19          |
|    time_elapsed         | 109         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.012462612 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0329     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00818    |
|    value_loss           | 0.0205      |
-----------------------------------------

Evaluation at step 40000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 295         |
|    ep_rew_mean          | 0.738       |
| time/                   |             |
|    fps                  | 329         |
|    iterations           | 20          |
|    time_elapsed         | 124         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.013528593 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.317       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0121      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.0245      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 113         |
|    ep_rew_mean          | 0.904       |
| time/                   |             |
|    fps                  | 334         |
|    iterations           | 21          |
|    time_elapsed         | 128         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.013822693 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.459       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.024      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.0259      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 63.3        |
|    ep_rew_mean          | 0.946       |
| time/                   |             |
|    fps                  | 339         |
|    iterations           | 22          |
|    time_elapsed         | 132         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.009625005 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.453       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00813     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.0246      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 48          |
|    ep_rew_mean          | 0.959       |
| time/                   |             |
|    fps                  | 344         |
|    iterations           | 23          |
|    time_elapsed         | 136         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.010054056 |
|    clip_fraction        | 0.095       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.493       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0166     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 0.0184      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 40.8        |
|    ep_rew_mean          | 0.965       |
| time/                   |             |
|    fps                  | 348         |
|    iterations           | 24          |
|    time_elapsed         | 140         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.012211193 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.978      |
|    explained_variance   | 0.504       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00278     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00663    |
|    value_loss           | 0.0122      |
-----------------------------------------

Evaluation at step 50000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 38.2        |
|    ep_rew_mean          | 0.967       |
| time/                   |             |
|    fps                  | 352         |
|    iterations           | 25          |
|    time_elapsed         | 145         |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.007461304 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.894      |
|    explained_variance   | 0.508       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0208     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00557    |
|    value_loss           | 0.00997     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 35.5       |
|    ep_rew_mean          | 0.97       |
| time/                   |            |
|    fps                  | 356        |
|    iterations           | 26         |
|    time_elapsed         | 149        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.00742409 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.541      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.000583   |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.00871   |
|    value_loss           | 0.00653    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 33.4        |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 359         |
|    iterations           | 27          |
|    time_elapsed         | 153         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.005719551 |
|    clip_fraction        | 0.0912      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.634      |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0166     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00537    |
|    value_loss           | 0.00516     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 31.9         |
|    ep_rew_mean          | 0.973        |
| time/                   |              |
|    fps                  | 363          |
|    iterations           | 28           |
|    time_elapsed         | 157          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0054401616 |
|    clip_fraction        | 0.0744       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.541       |
|    explained_variance   | 0.579        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00143     |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00523     |
|    value_loss           | 0.0044       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30.5        |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 366         |
|    iterations           | 29          |
|    time_elapsed         | 161         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.012014333 |
|    clip_fraction        | 0.0663      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.447      |
|    explained_variance   | 0.574       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0116     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00611    |
|    value_loss           | 0.0036      |
-----------------------------------------

Evaluation at step 60000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 42.5        |
|    ep_rew_mean          | 0.964       |
| time/                   |             |
|    fps                  | 348         |
|    iterations           | 30          |
|    time_elapsed         | 176         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.034393974 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.448      |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00857    |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00555    |
|    value_loss           | 0.0031      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 52          |
|    ep_rew_mean          | 0.956       |
| time/                   |             |
|    fps                  | 351         |
|    iterations           | 31          |
|    time_elapsed         | 180         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.035416037 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.819      |
|    explained_variance   | -0.136      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0559     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 0.00552     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 52.5        |
|    ep_rew_mean          | 0.955       |
| time/                   |             |
|    fps                  | 354         |
|    iterations           | 32          |
|    time_elapsed         | 184         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.012269272 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.893      |
|    explained_variance   | 0.305       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0182     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.00712     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 40          |
|    ep_rew_mean          | 0.966       |
| time/                   |             |
|    fps                  | 357         |
|    iterations           | 33          |
|    time_elapsed         | 188         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.019238815 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.725      |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0296     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.00704     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 34.3        |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 360         |
|    iterations           | 34          |
|    time_elapsed         | 193         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.013505088 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.564      |
|    explained_variance   | 0.459       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0104     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.00673     |
-----------------------------------------

Evaluation at step 70000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30.6        |
|    ep_rew_mean          | 0.974       |
| time/                   |             |
|    fps                  | 362         |
|    iterations           | 35          |
|    time_elapsed         | 197         |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.007341207 |
|    clip_fraction        | 0.071       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.39       |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00585    |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00561    |
|    value_loss           | 0.00426     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30.1         |
|    ep_rew_mean          | 0.974        |
| time/                   |              |
|    fps                  | 365          |
|    iterations           | 36           |
|    time_elapsed         | 201          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0044069337 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.315       |
|    explained_variance   | 0.587        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000564     |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00394     |
|    value_loss           | 0.00293      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.4        |
|    ep_rew_mean          | 0.976       |
| time/                   |             |
|    fps                  | 368         |
|    iterations           | 37          |
|    time_elapsed         | 205         |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.016630009 |
|    clip_fraction        | 0.0457      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.228      |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00261    |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0047     |
|    value_loss           | 0.00296     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.3         |
|    ep_rew_mean          | 0.976        |
| time/                   |              |
|    fps                  | 370          |
|    iterations           | 38           |
|    time_elapsed         | 210          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0074310023 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.151       |
|    explained_variance   | 0.6          |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00133      |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00101     |
|    value_loss           | 0.00246      |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.4        |
|    ep_rew_mean          | 0.975       |
| time/                   |             |
|    fps                  | 372         |
|    iterations           | 39          |
|    time_elapsed         | 214         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.032092817 |
|    clip_fraction        | 0.0466      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00205    |
|    n_updates            | 380         |
|    policy_gradient_loss | 0.00211     |
|    value_loss           | 0.00275     |
-----------------------------------------

Evaluation at step 80000: mean reward = 0.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.1         |
|    ep_rew_mean          | 0.976        |
| time/                   |              |
|    fps                  | 374          |
|    iterations           | 40           |
|    time_elapsed         | 218          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0019888373 |
|    clip_fraction        | 0.0334       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.187       |
|    explained_variance   | 0.598        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00082     |
|    n_updates            | 390          |
|    policy_gradient_loss | 0.00213      |
|    value_loss           | 0.0029       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 27.6         |
|    ep_rew_mean          | 0.976        |
| time/                   |              |
|    fps                  | 376          |
|    iterations           | 41           |
|    time_elapsed         | 222          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0033381574 |
|    clip_fraction        | 0.0531       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.606        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00457     |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00278     |
|    value_loss           | 0.00245      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.9         |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 378          |
|    iterations           | 42           |
|    time_elapsed         | 226          |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.0042917966 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.153       |
|    explained_variance   | 0.6          |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0033      |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00101     |
|    value_loss           | 0.00255      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.7         |
|    ep_rew_mean          | 0.975        |
| time/                   |              |
|    fps                  | 380          |
|    iterations           | 43           |
|    time_elapsed         | 231          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0053476957 |
|    clip_fraction        | 0.0663       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.222       |
|    explained_variance   | 0.583        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00474     |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00506     |
|    value_loss           | 0.00233      |
------------------------------------------

Evaluation at step 90000: mean reward = 0.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 28.1         |
|    ep_rew_mean          | 0.976        |
| time/                   |              |
|    fps                  | 382          |
|    iterations           | 44           |
|    time_elapsed         | 235          |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 0.0087220855 |
|    clip_fraction        | 0.0635       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.153       |
|    explained_variance   | 0.607        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00437      |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00364     |
|    value_loss           | 0.00251      |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 37.3       |
|    ep_rew_mean          | 0.968      |
| time/                   |            |
|    fps                  | 384        |
|    iterations           | 45         |
|    time_elapsed         | 239        |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.08380219 |
|    clip_fraction        | 0.0539     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.189     |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0125    |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.000991  |
|    value_loss           | 0.00283    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 45.6        |
|    ep_rew_mean          | 0.961       |
| time/                   |             |
|    fps                  | 385         |
|    iterations           | 46          |
|    time_elapsed         | 244         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.038232476 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.298       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0376     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.00563     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 44.3       |
|    ep_rew_mean          | 0.962      |
| time/                   |            |
|    fps                  | 387        |
|    iterations           | 47         |
|    time_elapsed         | 248        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.04630387 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.666     |
|    explained_variance   | 0.437      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0294    |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.00729   |
|    value_loss           | 0.00621    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 40          |
|    ep_rew_mean          | 0.965       |
| time/                   |             |
|    fps                  | 389         |
|    iterations           | 48          |
|    time_elapsed         | 252         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.011107672 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.747      |
|    explained_variance   | 0.493       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0222     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00604    |
|    value_loss           | 0.00525     |
-----------------------------------------

Evaluation at step 100000: mean reward = 0.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 38.6        |
|    ep_rew_mean          | 0.967       |
| time/                   |             |
|    fps                  | 390         |
|    iterations           | 49          |
|    time_elapsed         | 257         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.016719423 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.53        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0102     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00713    |
|    value_loss           | 0.00547     |
-----------------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 686      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 570         |
|    iterations           | 2           |
|    time_elapsed         | 7           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.006189237 |
|    clip_fraction        | 0.00952     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -6.46       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00212     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00171    |
|    value_loss           | 5.92e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 539         |
|    iterations           | 3           |
|    time_elapsed         | 11          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.009424395 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -2.3        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0352     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 1.48e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 4           |
|    time_elapsed         | 15          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.008661819 |
|    clip_fraction        | 0.0837      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | -1.11       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0356     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00884    |
|    value_loss           | 1.33e-05    |
-----------------------------------------

Evaluation at step 10000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 5           |
|    time_elapsed         | 30          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.018001392 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | -12.1       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0299     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 1.54e-05    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 0          |
| time/                   |            |
|    fps                  | 356        |
|    iterations           | 6          |
|    time_elapsed         | 34         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.01326187 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.85      |
|    explained_variance   | -4.89      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.032     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0121    |
|    value_loss           | 1.1e-05    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 370         |
|    iterations           | 7           |
|    time_elapsed         | 38          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.013063843 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | -6.1        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0412     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00891    |
|    value_loss           | 1.47e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 381         |
|    iterations           | 8           |
|    time_elapsed         | 42          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.016635215 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | -5.75       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0155     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 7.97e-06    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 0          |
| time/                   |            |
|    fps                  | 390        |
|    iterations           | 9          |
|    time_elapsed         | 47         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.01282854 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.85      |
|    explained_variance   | -4.75      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0425    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0191    |
|    value_loss           | 6.85e-06   |
----------------------------------------

Evaluation at step 20000: mean reward = 0.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 331         |
|    iterations           | 10          |
|    time_elapsed         | 61          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.014032376 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | -6.19       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0371     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 1.1e-05     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 340         |
|    iterations           | 11          |
|    time_elapsed         | 66          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.017388571 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | -3.1        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0334     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 6.99e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 349         |
|    iterations           | 12          |
|    time_elapsed         | 70          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.014576208 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | -4.21       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0309     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 1.03e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 357         |
|    iterations           | 13          |
|    time_elapsed         | 74          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.008935236 |
|    clip_fraction        | 0.0617      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -0.918      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0478     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00914    |
|    value_loss           | 2.03e-05    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 363         |
|    iterations           | 14          |
|    time_elapsed         | 78          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.013560307 |
|    clip_fraction        | 0.0991      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -1.56       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0243     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 8.39e-06    |
-----------------------------------------
Traceback (most recent call last):
  File "/home/research/lichengyu/intrinsic-rewards-wustl/Novelty_ds/train.py", line 47, in <module>
    df_episodic = train_agent(EpisodicCuriosityBonusWrapper)
  File "/home/research/lichengyu/intrinsic-rewards-wustl/Novelty_ds/train.py", line 32, in train_agent
    model.learn(total_timesteps=total_timesteps, callback=eval_callback)
  File "/home/research/lichengyu/miniconda3/envs/test/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/research/lichengyu/miniconda3/envs/test/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/research/lichengyu/miniconda3/envs/test/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 218, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/research/lichengyu/miniconda3/envs/test/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 222, in step
    return self.step_wait()
  File "/home/research/lichengyu/miniconda3/envs/test/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_monitor.py", line 76, in step_wait
    obs, rewards, dones, infos = self.venv.step_wait()
  File "/home/research/lichengyu/miniconda3/envs/test/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_transpose.py", line 97, in step_wait
    observations, rewards, dones, infos = self.venv.step_wait()
  File "/home/research/lichengyu/miniconda3/envs/test/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 59, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]
  File "/home/research/lichengyu/intrinsic-rewards-wustl/Novelty_ds/wrappers.py", line 63, in step
    self._train_r_model(X, y)
  File "/home/research/lichengyu/intrinsic-rewards-wustl/Novelty_ds/wrappers.py", line 96, in _train_r_model
    prob = self.r_model.get_label(obs1, obs2)
  File "/home/research/lichengyu/intrinsic-rewards-wustl/Novelty_ds/models.py", line 30, in get_label
    e1 = self.get_embedding(obs1)
  File "/home/research/lichengyu/intrinsic-rewards-wustl/Novelty_ds/models.py", line 26, in get_embedding
    obs = torch.FloatTensor(obs).permute(2,0,1).unsqueeze(0).to(device)
NameError: name 'torch' is not defined
